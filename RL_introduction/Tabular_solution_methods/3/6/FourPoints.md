### 1. 환경의 동역학 함수 $p$의 완벽한 이해 (The Dynamics Function)
이 섹션에서 가장 중요한 식은 **(3.6)**입니다.

$$p(s', r | s, a) \doteq \Pr\{S_{t+1}=s', R_{t+1}=r | S_t=s, A_t=a\}$$

*   **공부 포인트:**
    *   이 함수가 **'결합 확률 분포(Joint Probability Distribution)'**라는 점을 이해해야 합니다. 즉, 다음 상태($s'$)와 보상($r$)이 **동시에** 결정됩니다.
    *   이 함수 하나만 알면 환경의 모든 물리 법칙(확률적 특성 포함)을 다 아는 것과 같습니다.
    *   **질문해보기:** "왜 $p$ 함수에서 확률의 총합($\sum$)은 1이 되어야 하는가?"

### 2. 주변화(Marginalization)를 통한 파생 함수 유도 능력
기본 함수 $p(s', r | s, a)$로부터 우리가 자주 쓰는 다른 정보들을 끄집어낼 수 있어야 합니다.

*   **공부 포인트:**
    *   **상태 전이 확률 (State-transition probability):**
        *   $p(s'|s, a) = \sum_{r \in \mathcal{R}} p(s', r | s, a)$
        *   보상($r$)에 상관없이, $s$에서 $a$를 했을 때 $s'$로 갈 확률은 얼마인가? (보상에 대해 합을 구함)
    *   **기대 보상 (Expected rewards):**
        *   $r(s, a) = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r | s, a)$
        *   상태 $s$에서 행동 $a$를 하면 평균적으로 얼마의 보상을 받는가?

### 3. '유한(Finite)'의 의미와 '마르코프(Markov)'의 의미
*   **공부 포인트:**
    *   **Finite:** 상태(S), 행동(A), 보상(R)의 집합이 유한해야만 우리가 배울 알고리즘(표 형식, 행렬 연산 등)을 바로 적용할 수 있습니다.
    *   **Markov Property (복습):** 현재 상태 $s$와 행동 $a$만 있으면, 과거의 역사(History)는 미래를 예측하는 데 필요 없다는 가정입니다. MDP는 이 성질을 전제로 합니다.

### 4. [실전 감각] 재활용 로봇 예제로 MDP 모델링 해보기
이론을 실제 문제로 바꾸는 능력을 기르는 파트입니다.

*   **공부 포인트:**
    *   **상태 정의:** 로봇의 상태를 왜 '배터리 레벨(High/Low)'로만 정의했는가?
    *   **행동 정의:** 각 상태에서 가능한 행동 집합 $A(s)$가 왜 다른가? (Low일 때만 Recharge 가능)
    *   **전이 확률:** $\alpha$(High 유지 확률)와 $\beta$(Low 유지 확률) 같은 파라미터가 어떻게 $p$ 함수에 녹아드는가?
    *   **보상 설계:** 캔을 주울 때($r_{search}$), 대기할 때($r_{wait}$), 방전되어 구조될 때(-3)의 보상을 어떻게 수치화했는가?
    *   **PPT 팁:** Figure 3.3(전이 그래프)을 띄워놓고, 특정 화살표 하나를 찍어서 **"이 화살표는 수식으로 $p(\text{high}, r_{search} | \text{high}, \text{search}) = \alpha$를 의미합니다"**라고 설명할 수 있어야 합니다.
