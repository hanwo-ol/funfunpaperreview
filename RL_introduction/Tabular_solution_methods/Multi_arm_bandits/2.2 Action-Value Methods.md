단락 1
----
1. 실제 가치 vs. 추정 가치

*   **$q(a)$ (실제 가치, True Value):**
    *   행동 $a$가 가진 **객관적인, 진짜 평균 보상**
    *   이것은 환경에 의해 결정되는 값으로, 에이전트는 이 값을 모름

*   **$Q_t(a)$ (추정 가치, Estimated Value):**
    *   시점 $t$에서 에이전트가 경험을 바탕으로 **추정한** 행동 $a$의 가치
    *   이것은 에이전트의 **"지식"**이고, 학습을 통해 계속 업데이트될 거임
    *   학습의 목표는 $Q_t(a)$를 실제 값인 $q(a)$에 최대한 가깝게 만드는 것

2. 샘플-평균 방법 (Sample-Average Method)

*   어떤 행동의 평균적인 가치를 알고 싶다면, 그냥 그 행동을 여러 번 해보고 받은 보상들의 평균을 내면 되지 않을까?
*   **식 (2.1):**
    *   $Q_t(a) = \dfrac{\text{지금까지 행동 } a\text{를 통해 받은 모든 보상의 합}}{\text{지금까지 행동 } a\text{를 선택한 총 횟수}} =  \dfrac{R_1 + R_2 + \dots + R_{N_t(a)}}{N_t(a)}.$

3. 샘플-평균 방법의 속성

*   **초기값 문제 ($N_t(a)=0$):**
    *   아직 한 번도 선택해보지 않은 행동은 평균을 계산할 수 없음.
    *   이런 경우를 대비해, 모든 행동의 가치를 어떤 **기본값(default value)**으로 초기화하고, 가장 흔한 선택은 **0**임
    *   "아무 정보가 없으면, 일단 가치를 0으로 가정하자"

*   **수렴 보장 (Convergence Guarantee):**
    *   **대수의 법칙 (Law of Large Numbers):** 어떤 사건을 무한히 많이 반복하면 그 표본 평균은 실제 기댓값(평균)에 수렴한다
    *   **강화학습에의 적용:** 만약 우리가 모든 행동을 무한히 많이 선택한다면($N_t(a) \to \infty$), 샘플-평균 방법으로 계산한 추정 가치 $Q_t(a)$는 반드시 실제 가치 $q(a)$로 수렴하게 될것.
    *   -> 샘플-평균 방법이 이론적 타당성을 뒷받침 해줌

4. 아 잠깐만 근데...
*   환경이 변하는 비정상적인 문제에서는 오래된 과거의 경험과 최근의 경험을 동일하게 취급해야 해?
*   그래서, 우리가 이렇게 가치를 추정했다면, **그 추정치를 가지고 어떻게 다음 행동을 선택해야 하는가?**"

단락 2
----
가장 간단한 행동 선택 규칙은 추정된 행동 가치가 가장 높은 행동(또는 행동들 중 하나)을 선택하는 것이고,
단계 $t$에서 $Q_t(A_t^*) = \max_a Q_t(a)$를 만족하는 탐욕적 행동(greedy actions) $A_t^*$ 중 하나를 선택하는 것


greedy actions selection method: $A_t = \underset{a}{\operatorname{argmax}} Q_t(a)$

*   $t$: 현재 시점(time step)
*   $A_t$: 현재 시점 $t$에서 선택할 행동
*   $a$: 가능한 모든 행동의 집합에 속한 개별 행동
*   $Q_t(a)$: 시점 $t$에서 에이전트가 추정하는 행동 $a$의 **가치(Q-value)**. 즉, "이 행동을 하면 앞으로 얼마나 좋은 보상을 받을 것으로 기대되는가?"에 대한 현재의 추정치
*   $\underset{a}{\operatorname{argmax}}$: 괄호 안의 값을 최대로 만드는 행동 $a$를 찾자

**"가지고 있는 모든 행동 카드($a$) 중에서, 예상 점수($Q_t(a)$)가 가장 높은 카드를 뽑아라"**는 의미인 것 같음

greedy action 방법의 장점
*   단기적인 보상을 극대화하는 데 효과적
*   알고리즘이 단순하고 계산적으로 빠름

단점
*   탐욕적 방법은 **탐험을 전혀 하지 않함, 전혀.**
*   만약 초반에 우연히 얻은 나쁜 경험 때문에 **실제로는 가장 좋은 행동**의 추정 가치($Q_t$)가 낮게 형성되었다면, 에이전트는 그 행동을 다시는 시도해보지 않을 것임
*   그 결과, 최적의 해답을 찾지 못하고 차선책에 영원히 머물게 됨

대안: $\varepsilon$-탐욕적($\varepsilon$-greedy) 방법
*   이 방법은 탐욕적 방법의 단점을 보완하기 위해 아주 간단한 규칙을 추가함
*   대부분의 시간(확률 $1-\varepsilon$) 동안은 **탐욕적으로 행동** 하고
*   아주 가끔(작은 확률 $\varepsilon$)은 현재 추정 가치와 상관없이 **모든 행동 중 하나를 무작위로 선택**하도록 설계
*   단, $\varepsilon$(엡실론)은 0과 1 사이의 작은 값, 탐험의 정도를 조절하는 하이퍼파라미터

$\varepsilon$-greedy 방법의 장점
*   $N_t(a) \to \infty$: 시도 횟수 $t$가 무한대로 갈수록, 모든 행동 $a$의 선택 횟수($N_t(a)$) 또한 무한대로 발산함
*   $Q_t(a) \to q(a)$: 모든 행동을 무한히 샘플링하게 되므로, 에이전트의 추정 가치($Q_t(a)$)는 실제 평균 가치($q(a)$)에 수렴함

$\varepsilon$-greedy 방법의 한계
