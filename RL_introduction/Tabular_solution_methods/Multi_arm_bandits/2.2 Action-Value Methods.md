단락 1
----
1. 실제 가치 vs. 추정 가치

*   **$q(a)$ (실제 가치, True Value):**
    *   행동 $a$가 가진 **객관적인, 진짜 평균 보상**
    *   이것은 환경에 의해 결정되는 값으로, 에이전트는 이 값을 모름

*   **$Q_t(a)$ (추정 가치, Estimated Value):**
    *   시점 $t$에서 에이전트가 경험을 바탕으로 **추정한** 행동 $a$의 가치
    *   이것은 에이전트의 **"지식"**이고, 학습을 통해 계속 업데이트될 거임
    *   학습의 목표는 $Q_t(a)$를 실제 값인 $q(a)$에 최대한 가깝게 만드는 것

2. 샘플-평균 방법 (Sample-Average Method)

*   어떤 행동의 평균적인 가치를 알고 싶다면, 그냥 그 행동을 여러 번 해보고 받은 보상들의 평균을 내면 되지 않을까?
*   **식 (2.1):**
    *   $Q_t(a) = \dfrac{\text{지금까지 행동 } a\text{를 통해 받은 모든 보상의 합}}{\text{지금까지 행동 } a\text{를 선택한 총 횟수}} =  \dfrac{R_1 + R_2 + \dots + R_{N_t(a)}}{N_t(a)}.$

3. 샘플-평균 방법의 속성

*   **초기값 문제 ($N_t(a)=0$):**
    *   아직 한 번도 선택해보지 않은 행동은 평균을 계산할 수 없음.
    *   이런 경우를 대비해, 모든 행동의 가치를 어떤 **기본값(default value)**으로 초기화하고, 가장 흔한 선택은 **0**임
    *   "아무 정보가 없으면, 일단 가치를 0으로 가정하자"

*   **수렴 보장 (Convergence Guarantee):**
    *   **대수의 법칙 (Law of Large Numbers):** 어떤 사건을 무한히 많이 반복하면 그 표본 평균은 실제 기댓값(평균)에 수렴한다
    *   **강화학습에의 적용:** 만약 우리가 모든 행동을 무한히 많이 선택한다면($N_t(a) \to \infty$), 샘플-평균 방법으로 계산한 추정 가치 $Q_t(a)$는 반드시 실제 가치 $q(a)$로 수렴하게 될것.
    *   -> 샘플-평균 방법이 이론적 타당성을 뒷받침 해줌

4. 아 잠깐만 근데...
*   환경이 변하는 비정상적인 문제에서는 오래된 과거의 경험과 최근의 경험을 동일하게 취급해야 해?
*   그래서, 우리가 이렇게 가치를 추정했다면, **그 추정치를 가지고 어떻게 다음 행동을 선택해야 하는가?**"

단락 2
----
가장 간단한 행동 선택 규칙은 추정된 행동 가치가 가장 높은 행동(또는 행동들 중 하나)을 선택하는 것이고,
단계 $t$에서
$Q_t(A_t^*) = \max_a Q_t(a)$를 만족하는 탐욕적 행동(greedy actions) $A_t^*$ 중 하나를 선택하는 것


greedy actions selection method: $A_t = \underset{a}{\text{argmax}} Q_t(a)$

*   $t$: 현재 시점(time step)
*   $A_t$: 현재 시점 $t$에서 선택할 행동
*   $a$: 가능한 모든 행동의 집합에 속한 개별 행동
*   $Q_t(a)$: 시점 $t$에서 에이전트가 추정하는 행동 $a$의 **가치(Q-value)**. 즉, "이 행동을 하면 앞으로 얼마나 좋은 보상을 받을 것으로 기대되는가?"에 대한 현재의 추정치
*   $\underset{a}{\text{argmax}}$: 괄호 안의 값을 최대로 만드는 행동 $a$를 찾자

**"가지고 있는 모든 행동 카드($a$) 중에서, 예상 점수($Q_t(a)$)가 가장 높은 카드를 뽑아라"**는 의미인 것 같음

greedy action 방법의 장점
*   단기적인 보상을 극대화하는 데 효과적
*   알고리즘이 단순하고 계산적으로 빠름

단점
*   탐욕적 방법은 **탐험을 전혀 하지 않함, 전혀.**
*   만약 초반에 우연히 얻은 나쁜 경험 때문에 **실제로는 가장 좋은 행동**의 추정 가치($Q_t$)가 낮게 형성되었다면, 에이전트는 그 행동을 다시는 시도해보지 않을 것임
*   그 결과, 최적의 해답을 찾지 못하고 차선책에 영원히 머물게 됨

대안: $\varepsilon$-탐욕적($\varepsilon$-greedy) 방법
*   이 방법은 탐욕적 방법의 단점을 보완하기 위해 아주 간단한 규칙을 추가함
*   대부분의 시간(확률 $1-\varepsilon$) 동안은 **탐욕적으로 행동** 하고
*   아주 가끔(작은 확률 $\varepsilon$)은 현재 추정 가치와 상관없이 **모든 행동 중 하나를 무작위로 선택**하도록 설계
*   단, $\varepsilon$(엡실론)은 0과 1 사이의 작은 값, 탐험의 정도를 조절하는 하이퍼파라미터

$\varepsilon$-greedy 방법의 장점
*   $N_t(a) \to \infty$: 시도 횟수 $t$가 무한대로 갈수록, 모든 행동 $a$의 선택 횟수($N_t(a)$) 또한 무한대로 발산함
*   $Q_t(a) \to q(a)$: 모든 행동을 무한히 샘플링하게 되므로, 에이전트의 추정 가치($Q_t(a)$)는 실제 평균 가치($q(a)$)에 수렴함

$\varepsilon$-greedy 방법의 한계??
*   "점근적" 보장이므로, 이는 **이론적으로 무한한 시간**이 주어졌을 때 올바른 값으로 수렴한다는 의미임
*   실제 문제에서는 유한한 시간과 데이터만 주어짐. 따라서 이 방법이 얼마나 **빨리** 좋은 정책을 학습하는지, 또는 특정 $\varepsilon$ 값이 주어진 문제에 최적인지는 우리가 직접 실행해보고 시행착오를 겪어야 할 듯.

단락 3
----

탐욕적 방법과 $\varepsilon$-탐욕적 방법의 성능을 객관적으로 비교하기 위해 설계한 **실험**

1. 실험의 목적
*   **"어떤 전략이 더 나은가?"**
*   이론적으로는 $\varepsilon$-탐욕적 방법이 수렴을 보장하지만, 실제 유한한 시간 내에서는 탐욕적 방법이 더 나을 수도 있음

2. 실험 환경: "10-armed testbeds"

*   **문제의 수:** 총 2000개의 서로 다른 밴딧을 생성
    *   **이유:** 단 하나의 문제로 실험하면, 그 결과가 특정 문제에만 국한된 우연일 수 있음

*   **각 문제의 구성 ($n=10$):**
    *   각 밴딧 문제는 10개의 팔(행동)을 가짐. 즉, 에이전트는 매 순간 10개의 선택지 중 하나를 골라야 함
    *   **행동의 실제 가치 ($q(a)$) 설정:** 10개 팔의 실제 가치(평균 보상)는 평균이 0이고 분산이 1인 정규분포(가우시안 분포)에서 무작위로 추출
        *   **의미:** 각 밴딧 문제마다 어떤 팔이 좋은지(가치가 높은지), 어떤 팔이 나쁜지(가치가 낮은지)가 달라
        *   또한 팔들의 가치가 서로 비슷할 수도 있고, 특정 팔 하나만 월등히 좋을 수도 있는 등 다양한 상황이 만들어짐
        *   진짜 도박같네
     
*   **보상($R_t$)의 결정 방식:**
    *   에이전트가 $t$ 시점에 팔 $A_t$를 선택하면, 받는 보상 $R_t$는 다음과 같이 결정됨
    
        $R_t = q(A_t) + \text{Noise}$
    
    *   $q(A_t)$: 선택한 팔 $A_t$의 **실제 평균 가치**
    *   $\text{Noise}$: 평균 0, 분산 1의 정규분포에서 추출된 **무작위 노이즈**
        *  잘 보면 좋은 팔을 선택했다고 해서 항상 좋은 보상을 받는 것이 아님. 운이 나쁘면 좋은 팔에서도 낮은 보상을 받을 수 있고, 반대로 나쁜 팔에서도 운 좋게 높은 보상을 받을 수 있음.
        *  에이전트가 한두 번의 경험으로 판단하지 않고 여러 번 시도해서 해당 팔의 가치를 파악하도록 만드는 것 같음

3. 성능 평가
*   **실험 진행:** 각 2000개의 밴딧 문제에 대해, 각 알고리즘(탐욕적, $\varepsilon$-탐욕적 등)을 1000번의 시도(steps) 동안 실행
*   **데이터 수집:** 매 시도마다 어떤 행동을 선택했고, 어떤 보상을 받았는지 기록
*   **평균화:** 2000개 문제 전체에 대한 성능을 평균하여 하나의 그래프로 나타냄.
*   **결과 시각화 (그림 2.1):** 이 평균화된 결과를 그래프로 그려, 시간(steps)이 지남에 따라 각 알고리즘의 성능이 어떻게 변화하는지를 시각적으로 비교합


단락 4
-----
<img width="549" height="685" alt="image" src="https://github.com/user-attachments/assets/b128147a-e000-4840-8923-7663e3347b0c" />

**탐욕적 방법 ($\varepsilon=0$)**

*   **초기 성능:** 학습 초반에는 가장 빠르게 성능이 향상. 무작위 탐험에 선택 할애않고 현재까지 얻은 정보로 가장 좋아 보이는 것을 즉시 활용하기 때문
*   **장기 성능:** 곧 성능 향상이 멈추고 낮은 수준에 머무름
    *   **이유:** 초반에 우연히 얻은 나쁜 경험으로 인해 실제 최적의 행동을 저평가하게 되면, 다시는 그 행동을 시도하지 않음
    *   **평균 보상:**
        *   단계당 평균 보상이 약 1 (최적은 1.55).
        *   전체 2000개의 문제 중 약 3분의 1(약 667개)에서만 최적 행동을 찾았고, 나머지 3분의 2(약 1333개)에서는 영원히 차선의 행동에 머무름

**$\varepsilon$-탐욕적 방법**

*   **성능:** 지속적인 탐험 덕분에, 탐욕적 방법과 달리 꾸준히 성능이 개선되어 결국 더 높은 보상을 얻음

*   **$\varepsilon = 0.1$ (높은 탐험 비율):**
    *   **장점:** 더 자주 탐험하므로, 최적의 행동을 **더 빨리** 찾아내고 있음. 심지어 학습 초반에 성능이 빠르게 오름
    *   **단점:** 학습이 충분히 진행된 후에도 10%의 확률로 계속 무작위 행동을 함. 최적의 행동을 찾았더라도, 그 행동을 91%($1 - \varepsilon + \varepsilon/10 = 1 - 0.1 + 0.01 = 0.91$) 이상의 확률로는 선택하지 못함. 성능의 상한선이 생길 것

*   **$\varepsilon = 0.01$ (낮은 탐험 비율):**
    *   **장점:** 학습이 충분히 진행된 후에는 99%의 시간 동안 최적 행동을 활용할 수 있으므로, **장기적으로는 $\varepsilon=0.1$보다 더 높은 성능(더 높은 평균 보상, 더 높은 최적 행동 선택률)을 달성**
    *   **단점:** 탐험 빈도가 낮아 최적의 행동을 찾아내는 데 시간이 더 오래 걸림. 내년에나 최적의 행동을 찾을 것 같음. 학습 초반의 성능 개선 속도가 더

트레이드 오프가 있음


단락 5
-----

1. 문제의 불확실성과 그에 따른 탐험률(?)을 어떻게 해야 할까

*   **보상 분산이 클 때 (Noisy Rewards):**
    *   **상황:** 보상의 무작위성이 매우 클 때 (분산=10)
    *   한두 번의 시도로는 행동의 진짜 평균 가치를 파악하기 어려움. 우연히 좋은 행동에서 나쁜 보상을 받거나, 나쁜 행동에서 좋은 보상을 받을 가능성이 있기 때문임
    *   **판단?** 최적 행동을 확실히 찾아내려면 **더 많은 탐험**이 필수적임. 따라서 $\varepsilon$-탐욕적 방법의 가치가 더욱 커짐

*   **보상 분산이 0일 때 (Deterministic Rewards):**
    *   **상황:** 보상이 완전히 결정론적일 때. 즉, 특정 행동은 항상 정확히 동일한 보상을 줄 때
    *   이 경우, 에이전트는 각 행동을 단 한 번만 시도해보면 그 행동의 참 가치를 완벽하게 알 수 있음
    *   **판단?** 탐욕적 방법을 써야 함. 모든 행동을 한 번씩만 시도해보고, 그중 가장 좋은 것을 찾은 뒤에는 더 이상 불필요한 탐험을 할 필요가 없어짐

2. 문제의 비정상성(Nonstationarity)

*   **비정상적 문제란?**
    *   행동의 실제 가치 $q(a)$가 시간에 따라 변하는 문제
    *   특정 광고 전략의 효과가 유행에 따라 변하거나, 경쟁사의 등장으로 인해 특정 상품의 수익성이 변하는 경우가 허다함

*   **비정상성 하에서의 탐험:**
    *   보상이 결정론적이더라도 (분산=0), 문제가 비정상적이라면 탐험은 **필수적**이게 될 것임
    *   **왜???** 현재 가장 좋다고 알려진 탐욕적 행동(greedy action)이 시간이 지나면서 최적이 아니게 될 수 있음
    *   과거에는 별로였던 다른 행동의 가치가 상승해서 새로운 최적의 행동이 되었을 수 있기 때문
    *   지속적인 탐험 없이는 이러한 변화를 감지할 방법이 없으니, 모델을 괴롭혀야 함
