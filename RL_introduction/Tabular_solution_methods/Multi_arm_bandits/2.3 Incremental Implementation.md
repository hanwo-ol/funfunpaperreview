단락 1
----

지금까지 논의한 행동-가치 방법들은 모두 관찰된 보상들의 **샘플 평균(sample averages)**으로 행동 가치를 추정했음
*  이를 구현하는 가장 명백한 방법은 각 행동 $a$에 대해, 그 행동이 선택된 이후에 뒤따랐던 모든 보상들의 기록을 유지하는 거임
*  그리고 시간 $t$에 행동 $a$의 가치 추정치가 필요할 때, 아래 식에 따라 계산하는데,

$Q_t(a) = \frac{R_1 + R_2 + \dots + R_{N_t(a)}}{N_t(a)}$,

여기서 $R_1, \dots, R_{N_t(a)}$는 시점 $t$ 이전에 행동 $a$를 선택했을 때 받았던 모든 보상들을 의미하고 있음
* 이 직관적인 구현 방식의 **문제점**은 **메모리와 계산 요구량이 시간의 흐름에 따라 무한정 증가**한다는 것
* 즉, 행동 $a$를 선택한 후 추가적인 보상이 하나 더 발생할 때마다, 이를 저장하기 위해 더 많은 메모리가 필요하고, $Q_t(a)$를 결정하기 위해 더 많은 계산이 요구될 것임.

단락 2
-----

위의 식처럼 모든 보상을 저장하는건 필요하지 않음 ㅋ
* 새로운 보상이 들어올 때마다 적고 일정한 계산량으로 평균을 계산하는 **점진적 업데이트 공식(incremental update formulas)** 을 이용하면 되기 때문임
* 어떤 행동에 대해, $Q_k$를 그것의 $k$번째 보상에 대한 추정치, 즉 그것의 첫 $k-1$개 보상들의 평균이라고 하고,
*  이 평균값과 해당 행동에 대한 $k$번째 보상 $R_k$가 주어졌을 때, 모든 $k$개 보상들의 평균은 다음과 같이 계산할 수 있음.

<img width="277" height="197" alt="image" src="https://github.com/user-attachments/assets/700731cf-c4ce-4d41-bfd5-f0ae19aa964a" />

이 구현 방식은 오직 $Q_k$와 $k$를 위한 메모리만을 요구하니까 작은 계산량만 필요해짐

좀 더 자세히 풀면

*   **1단계:** $Q_{k+1} = \frac{1}{k} \sum_{i=1}^{k} R_i$
    *    $k$개의 보상에 대한 새로운 평균($Q_{k+1}$)은, 1번째부터 $k$번째까지의 모든 보상($R_i$)을 더한 후 $k$로 나눈 값

*   **2단계:** $= \frac{1}{k} \left( R_k + \sum_{i=1}^{k-1} R_i \right)$
    *    $k$개의 합을 마지막 항($R_k$)과 나머지 $k-1$개의 합으로 분리

*   **3단계:** $= \frac{1}{k} \left( R_k + (k-1)Q_k + Q_k - Q_k \right)$
    *    $Q_k$는 첫 $k-1$개 보상의 평균이었으므로, $\sum_{i=1}^{k-1} R_i = (k-1)Q_k$. 이 관계를 이용해 $\sum_{i=1}^{k-1} R_i$를 $(k-1)Q_k$로 대체
        *   수식을 정리하기 위해 $Q_k$를 더하고 빼는 과정(+Q_k - Q_k)을 사용

*   **4단계:** $= \frac{1}{k} \left( R_k + kQ_k - Q_k \right)$
    *    $(k-1)Q_k$와 $+Q_k$를 합쳐 $kQ_k$로 만듦

*   **5단계:** $= Q_k + \frac{1}{k} [R_k - Q_k]$
    *    괄호 안의 항들을 $k$로 나누어 최종 형태로 정리
 
*   **$Q_{k+1}$ (새로운 추정치):** 우리가 계산하려는 새로운 평균값
*   **$Q_k$ (기존 추정치):** 우리가 이미 가지고 있는 이전까지의 평균값
*   **$R_k$ (새로운 정보):** 새롭게 관찰된 보상 값
*   **$[R_k - Q_k]$ (오차, Error):** 새롭게 얻은 정보($R_k$)가 기존의 평균적인 기대($Q_k$)와 얼마나 다른지를 나타내는 **예측 오차(prediction error)**
*   **$\frac{1}{k}$ (학습률, Step-size):** 이 오차를 얼마나 반영하여 기존 추정치를 수정할지를 결정하는 가중치
    *   시도 횟수 $k$가 커질수록 이 값은 작아짐
 
단락 3-4
-----

방금 본 업데이트 규칙은 간단하게 아래처럼 쓸 수 있음.
* 새로운 추정치(NewEstimate) $\leftarrow$ 기존 추정치(OldEstimate) + 단계 크기(StepSize) [목표(Target) - 기존 추정치(OldEstimate)]

또, 점진적 방법(incremental method)에서 사용된 단계 크기 파라미터(step-size parameter, *StepSize*)는 매 시점(time step)마다 변하고 있는 것을 볼 수 있는데,
* 학습 초반에는 경험이 적으므로 새로운 정보를 크게 반영하여(높은 $\alpha$) 추정치를 빠르게 수정하고
* 학습이 진행되어 경험이 많이 쌓일수록, 추정치는 이미 꽤 정확해졌다고 가정하므로 새로운 정보 하나에 의해 값이 크게 변하지 않도록 작은 값으로 반영할 수 있는 구조임.


  
