Multi-armed bandit?
---

여러 선택지 중 하나를 반복적으로 골라 장기적인 보상을 최대화해야 하는 상황을 다루는 문제임.
- 강화학습의 딜레마를 보여주는 문제.

---

강화학습이 다른 학습 유형과 구별되는 가장 중요한 특징?
+ 행동을 직접적으로 제시 X
+ 취한 행동을 평가하는 훈련 정보를 사용함(Evaluative Feedback)
  + 더 좋은 행동을 찾기 위해 시행착오를 탐색하는 exploration이 필요함ulti-armed bandit?
---

여러 선택지 중 하나를 반복적으로 골라 장기적인 보상을 최대화해야 하는 상황을 다루는 문제임.
- 강화학습의 딜레마를 보여주는 문제.

---

강화학습이 다른 학습 유형과 구별되는 가장 중요한 특징?
+ 행동을 직접적으로 제시 X
+ 취한 행동을 평가하는 훈련 정보를 사용함(Evaluative Feedback)
  + 더 좋은 행동을 찾기 위해 시행착오를 탐색하는 exploration이 필요함
  + 그럼 Evaluative Feedback과 Instructive Feedback의 차이는 무엇?

---

평가적 피드백 (Evaluative Feedback): "점수 매기기"

**평가적 피드백**은 에이전트(학습자)가 한 행동이 **얼마나 좋았는지 나빴는지만 알려주는** 방식

* **무엇을 알려주는가?**: "네가 방금 한 행동의 점수는 70점이야." 
* **무엇을 알려주지 않는가?**: "100점을 받으려면 어떻게 해야 했는지", 또는 "그 행동이 최선이었는지"는 알려주지 않음. 70점이 좋은 점수인지 나쁜 점수인지도 다른 행동과 비교해봐야 알 수 있음

위 같은 로직 때문에 **강화학습**에서는 **탐험(Exploration)**과 **시행착오(trial-and-error)**가 필수적이게 됨. 
더 높은 점수를 받기 위해, 에이전트는 스스로 다른 행동들을 시도해보고 그 결과(점수)를 비교하며 더 나은 행동을 찾아나가야 함.


지시적 피드백 (Instructive Feedback): "정답 알려주기"

**지시적 피드백**은 에이전트가 어떤 행동을 했는지와 상관없이 **무엇이 '정답'인지를 직접적으로 알려주는** 방식

* **무엇을 알려주는가?**: "그 상황에서는 이 행동을 했어야 해." 
* **특징**: 에이전트가 실제로 어떤 행동을 했는지는 중요하지 않음. 오직 '정답' 행동이 무엇인지만 전달됩니다.

이 방식은 **지도 학습(Supervised Learning)**의 기초임.
+ 예를 들어, 고양이 사진을 보여주고 '고양이'라는 정답(레이블)을 알려주며 학습시키는 것
+ 에이전트는 정답을 따라 하도록 학습하기 때문에 탐험이 필요 없습니다.

| 구분 | **평가적 피드백 (강화학습)** | **지시적 피드백 (지도학습)** |
| :--- | :--- | :--- |
| **피드백의 성격** | 행동에 대한 **점수** 또는 **보상**  | 정해진 **정답** |
| **에이전트의 과제** | 스스로 더 나은 행동을 **탐색** | 제시된 정답을 **모방** |
| **핵심 챌린지** | 탐험과 이용의 균형 | 데이터의 양과 질 |
| **행동과의 관계** | 내가 한 행동에 **전적으로 의존** | 내가 한 행동과 **독립적** |

---

이번 장에서는 강화학습의 evaluative 측면을 단순화된 환경에서 연구함.
+ 단순한 환경? 하나의 상황만 주어짐

비연관적(nonassociative) 환경?
+ 평가적 피드백을 포함하는 대부분의 이전연구가 수행된 환경임.
+ 강화학습 문제의 많은 복잡성을 피함.
+ n-armed bandit 문제를 다룰 것(단순화된 버전)

비연관적?
1. 비연관적(Nonassociative) 문제: 단 하나의 상황

**비연관적 문제**는 학습자가 **오직 하나의 상황(situation) 안에서만 행동하는 법을 배우는 단순화된 환경**을 말함

**핵심 특징**: 상황이 변하지 않음. 에이전트는 여러 상황에 따라 행동을 다르게 할 필요 없이, 주어진 단 하나의 상황에서 어떤 행동이 최선인지만 알아내면 됨
* **예?**: **Multi-armed Bandit** 문제
    * 카지노에 여러 슬롯머신이 있다고 하면, 내가 할 일은 이 머신들 중에서 돈을 가장 많이 주는 **최고의 머신 하나를 찾는 것**이 됨.
    * 방의 조명이 바뀌거나, 요일이 바뀐다고 해서 머신의 확률이 변하지 않음. 즉, **상황은 항상 동일**.
* **에이전트의 목표**: 여러 행동(슬롯머신 레버 당기기) 중 가장 보상이 높은 **단일 최선 행동(single best action)**을 찾는 것.

이처럼 상황을 고려할 필요가 없기 때문에 '비연관적'이라고 부르며, 완전한 강화학습 문제의 복잡성을 상당 부분 피할 수 있음.

**비연관적 문제**는 '탐험과 이용'이라는 강화학습의 근본적인 딜레마를 다른 복잡한 요소 없이 순수하게 연구할 수 있게 해주는 이상적인 환경이어서 중요한건가?
