단락 1
------------

이제 시간에 따라 밴딧 문제가 변함 -> 이전 절 까지 다뤄온 평균을 취하는 방법은 적절하지 않을 수 있음
* 실질적으로 비정상적인(effectively nonstationary) 강화학습 문제들이 대부분일 것임
* 오래된 보상들보다 최근의 보상들에 더 큰 가중치를 두는 것이 합리적임
* 가장 대중적인 방법 중 하나는 **고정된 단계 크기 파라미터(constant step-size parameter)**를 사용하는 것

그럼 아래의 식이
<img width="277" height="197" alt="image" src="https://github.com/user-attachments/assets/fcdd357a-7be3-4801-a5e7-f285a35b2f02" />

* 해당 식처럼 수정될 것임.   
$Q_{k+1} = Q_k + \alpha [R_k - Q_k]$
*   단계 크기 파라미터 $\alpha \in (0, 1]^1$는 상수

계속 풀어써보면

<img width="354" height="181" alt="image" src="https://github.com/user-attachments/assets/742c8ef8-9fbd-4f61-9fa8-814a0682c96e" />

 $Q_{k+1}$은 과거 보상들과 초기 추정치 $Q_1$의 가중 평균(weighted average)임을 알 수 있음
 
---

단락 2
----

#### 배경: 왜 수렴 조건이 필요한가?

강화학습에서 가치 추정치는 새로운 경험(보상)에 따라 계속 업데이트됩니다. 이때 단계 크기 $\alpha$가 업데이트의 크기를 조절합니다. 만약 $\alpha$를 잘못 설정하면, 추정치는 다음과 같은 문제가 발생할 수 있습니다.

*   **발산(Divergence):** 올바른 값에 가까워지는 것이 아니라 점점 더 멀어집니다.
*   **진동(Oscillation):** 올바른 값 주변에서 계속 심하게 위아래로 흔들립니다.
*   **잘못된 값으로의 수렴:** 실제 정답이 아닌 엉뚱한 값에 수렴하여 학습이 멈춥니다.

따라서, "어떻게 $\alpha$를 설정해야만 이러한 문제 없이 반드시 올바른 값으로 수렴한다고 보장할 수 있는가?"라는 질문에 답하는 것이 중요합니다.

#### 수렴을 위한 두 가지 조건 (식 2.7)

확률적 근사 이론은 $\alpha_k$ (k번째 업데이트 시점의 단계 크기)가 다음 두 가지 조건을 모두 만족하면, 추정치가 확률 1로 (즉, 거의 확실하게) 실제 값으로 수렴함을 보여줍니다.

**1. 첫 번째 조건: $\sum_{k=1}^{\infty} \alpha_k(a) = \infty$**

*   **수학적 의미:** 단계 크기의 합이 무한대로 발산해야 합니다.
*   **직관적 의미:** **"학습을 멈추지 않는다."**
    *   이 조건은 학습 단계가 너무 빨리 작아지는 것을 막습니다. 만약 단계 크기의 합이 유한한 값(예: 10)으로 수렴한다면, 아무리 많은 데이터를 처리해도 추정치는 총 10만큼의 크기 이상으로 변할 수 없게 됩니다.
    *   이 경우, 만약 초기 추정치가 실제 값과 10 이상 차이가 나거나, 학습 초반에 큰 무작위 변동(노이즈)이 있었다면, 알고리즘은 그 초기 오류를 끝까지 극복하지 못하고 잘못된 값에 갇히게 됩니다.
    *   따라서 이 조건은 **어떤 나쁜 초기 상태에서 시작하더라도 결국에는 올바른 방향으로 나아갈 수 있는 충분한 힘(업데이트 총량)을 보장**하는 역할을 합니다.
*   **예시:**
    *   $\alpha_k = 1/k$: 이 수열의 합(조화급수)은 무한대로 발산하므로 **조건을 만족합니다.**
    *   $\alpha_k = 1/k^2$: 이 수열의 합은 $\pi^2/6$으로 수렴하므로 **조건을 만족하지 못합니다.** 이 학습률을 사용하면 수렴을 보장할 수 없습니다.

**2. 두 번째 조건: $\sum_{k=1}^{\infty} \alpha_k^2(a) < \infty$**

*   **수학적 의미:** 단계 크기의 제곱의 합은 유한한 값으로 수렴해야 합니다.
*   **직관적 의미:** **"결국에는 충분히 안정된다."**
    *   이 조건은 학습 단계가 충분히 빠르게 작아져서, 나중에 발생하는 무작위 노이즈에 의해 추정치가 더 이상 크게 흔들리지 않도록 보장합니다.
    *   만약 이 조건이 만족되지 않으면(예: $\alpha_k$가 충분히 빨리 0에 가까워지지 않으면), 보상에 포함된 노이즈 때문에 추정치가 올바른 값에 도달했음에도 불구하고 계속해서 그 주변을 크게 맴돌며 안정되지 못할 수 있습니다.
    *   따라서 이 조건은 **학습이 충분히 진행된 후에는 변동성이 줄어들어 안정적으로 특정 값에 수렴할 수 있도록 보장**하는 역할을 합니다.
*   **예시:**
    *   $\alpha_k = 1/k$: 이 수열의 제곱의 합($\sum 1/k^2$)은 $\pi^2/6$으로 수렴하므로 **조건을 만족합니다.**
    *   $\alpha_k = 1/\sqrt{k}$: 이 수열의 제곱의 합($\sum 1/k$)은 무한대로 발산하므로 **조건을 만족하지 못합니다.**

**결론적으로,** 샘플-평균 방법에서 사용하는 단계 크기인 $\alpha_k = 1/k$는 이 두 가지 조건을 모두 만족하므로, 이론적으로 실제 평균 가치로의 수렴이 보장되는 훌륭한 선택입니다. 반면, 비정상 문제를 추적하기 위한 상수 단계 크기($\alpha_k = \alpha$)는 두 번째 조건을 만족하지 못하므로, 추정치가 특정 값으로 완전히 수렴하지 않고 계속해서 최근 보상에 따라 변동하게 됩니다.
