단락 1
------------

이제 시간에 따라 밴딧 문제가 변함 -> 이전 절 까지 다뤄온 평균을 취하는 방법은 적절하지 않을 수 있음
* 실질적으로 비정상적인(effectively nonstationary) 강화학습 문제들이 대부분일 것임
* 오래된 보상들보다 최근의 보상들에 더 큰 가중치를 두는 것이 합리적임
* 가장 대중적인 방법 중 하나는 **고정된 단계 크기 파라미터(constant step-size parameter)**를 사용하는 것

그럼 아래의 식이    
<img width="277" height="197" alt="image" src="https://github.com/user-attachments/assets/fcdd357a-7be3-4801-a5e7-f285a35b2f02" />

* 해당 식처럼 수정될 것임.   
$Q_{k+1} = Q_k + \alpha [R_k - Q_k]$
*   단계 크기 파라미터 $\alpha \in (0, 1]^1$는 상수

계속 풀어써보면

<img width="354" height="181" alt="image" src="https://github.com/user-attachments/assets/742c8ef8-9fbd-4f61-9fa8-814a0682c96e" />

 $Q_{k+1}$은 과거 보상들과 초기 추정치 $Q_1$의 가중 평균(weighted average)임을 알 수 있음
 
---

단락 2 -- 공부중
----
저자는 파라미터를 매 단계마다 변화시키는 것이 편리할거라고 하고 있음. 



위 식을 보면 가치 추정치는 새로운 보상에 따라 계속 업데이트되는데, 이때 stepsize인 단계 크기 $\alpha$가 업데이트의 크기를 조절하는 걸 볼 수 있음   
+ 이 때, 만약 $\alpha$를 잘못 설정하면, 가치에 대한 추정치는 다음과 같은 문제가 발생할 수 있음.

  *   **발산(Divergence):** 올바른 값에 가까워지는 것이 아니라 점점 더 멀어짐
  *   **진동(Oscillation):** 올바른 값 주변에서 계속 심하게 위아래로 흔들림
  *   **잘못된 값으로의 수렴:** 실제 정답이 아닌 엉뚱한 값에 수렴

따라서, "어떻게 $\alpha$를 설정해야 이러한 문제 없이 반드시 올바른 값으로 수렴한다고 보장할 수 있는가?"라는 질문이 나올것임

---


수렴을 위한 두 가지 조건 (식 2.7)

<img width="632" height="72" alt="image" src="https://github.com/user-attachments/assets/3d92bbee-1e65-4626-8e4a-de0ddaee626d" />

아래는 내가 해당 식을 이용한 방식임.

**1. 첫 번째 조건: $\sum_{k=1}^{\infty} \alpha_k(a) = \infty$**

*   **수학적 의미:** stepsize의 합이 무한대로 발산해야 함
*   **직관적 의미:** **"학습을 멈추지 않는다."**
    *   이 조건은 학습 단계가 너무 빨리 작아지는 것을 막음.
    *   만약 단계 크기의 합이 유한한 값(예: 10)으로 수렴한다면, 아무리 많은 데이터를 처리해도 추정치는 총 10만큼의 크기 이상으로 변할 수 없다는 의미
    *   따라서 이 조건은 **어떤 나쁜 초기 상태에서 시작하더라도 결국에는 올바른 방향으로 나아갈 수 있는 충분한 힘(업데이트 총량)을 보장**하는 역할을 합니다.
*   **예시:**
    *   $\alpha_k = 1/k$: 이 수열의 합(조화급수)은 무한대로 발산하므로 **조건을 만족**
    *   $\alpha_k = 1/k^2$: 이 수열의 합은 $\pi^2/6$으로 수렴하므로 **조건을 만족하지 못함**

**2. 두 번째 조건: $\sum_{k=1}^{\infty} \alpha_k^2(a) < \infty$**

*   **수학적 의미:** 단계 크기의 제곱의 합은 유한한 값으로 수렴해야 함
*   **직관적 의미:** **"결국에는 충분히 안정된다."**
    *   이 조건은 학습 단계가 충분히 빠르게 작아져서, 나중에 발생하는 무작위 노이즈에 의해 추정치가 더 이상 크게 흔들리지 않도록 보장하도록 함
    *   만약 이 조건이 만족되지 않으면(예: $\alpha_k$가 충분히 빨리 0에 가까워지지 않으면), 보상에 포함된 노이즈 때문에 추정치가 올바른 값에 도달했음에도 불구하고 계속해서 그 주변을 크게 맴돌며 안정되지 못할 수 있음
    *   따라서 이 조건은 **학습이 충분히 진행된 후에는 변동성이 줄어들어 안정적으로 특정 값에 수렴할 수 있도록 보장**하는 역할을 함
*   **예시:**
    *   $\alpha_k = 1/k$: 이 수열의 제곱의 합($\sum 1/k^2$)은 $\pi^2/6$으로 수렴하므로 **조건을 만족합니다.**
    *   $\alpha_k = 1/\sqrt{k}$: 이 수열의 제곱의 합($\sum 1/k$)은 무한대로 발산하므로 **조건을 만족하지 못합니다.**

**결론적으로,** 샘플-평균 방법에서 사용하는 단계 크기인 $\alpha_k = 1/k$는 이 두 가지 조건을 모두 만족하므로, 이론적으로 실제 평균 가치로의 수렴이 보장되는 설계임. 

반면, 상수로 단계 크기($\alpha_k = \alpha$)를 설정하는 방식은 두 번째 조건을 만족하지 못하므로, 추정치가 특정 값으로 완전히 수렴하지 않고 계속해서 최근 보상에 따라 변동하게 될 것 같음


