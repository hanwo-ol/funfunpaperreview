### 3.6 마르코프 결정 과정 (Markov Decision Processes)

**마르코프 성질(Markov property)** 을 만족하는 강화 학습 태스크를 **마르코프 결정 과정**, 줄여서 **MDP**라고 부릅니다. 
* 만약 상태(state)와 행동(action) 공간이 유한하다면, 이를 **유한 마르코프 결정 과정(finite MDP)** 이라고 부릅니다.
* 이 책 전반에 걸쳐 이를 광범위하게 다루는데, 유한 MDP만으로도 현대 강화 학습의 90%를 이해하는 데 충분하기 때문입니다.

특정한 유한 MDP는 상태 및 행동의 집합, 그리고 환경의 **한 단계 동역학(one-step dynamics)**
에 의해 정의됩니다. 
임의의 상태와 행동가 주어졌을 때, 발생 가능한 모든 다음 상태 $s'$와 보상의 쌍에 대한 확률은 다음과 같이 표기합니다.

이러한 값들은 유한 MDP의 동역학을 완벽하게 명시합니다. 이 책의 나머지 부분에서 제시하는 대부분의 이론은 환경이 유한 MDP라고 암묵적으로 가정하고 있습니다.

---

**마르코프 성질(Markov Property)**
은 강화 학습과 MDP(Markov Decision Process)의 근간이 되는 핵심 개념으로, 
간단히 요약하면 **"미래는 오직 현재 상태에 의해서만 결정되며, 과거의 역사는 영향을 주지 않는다"**
는 성질입니다.


### 1. 마르코프 성질의 정의 (Definition)

확률론적 과정에서 어떤 상태 $S_{t}$가 마르코프 성질을 갖는다는 것은, 
**현재의 상태($S_{t}$)와 행동($A_{t}$)을 알면, 그 이전의 모든 과거 정보(History)를 아는 것과 동일하게 미래를 예측할 수 있다**는 뜻입니다.

이를 수식으로 표현하면 다음과 같습니다.

$$\mathbb{P}[S_{t+1} | S_t] = \mathbb{P}[S_{t+1} | S_t, S_{t-1}, S_{t-2}, \dots, S_0]$$

* **좌변:** 현재 시점($t$)의 상태($S_{t}$)와 행동($A_{t}$)만 주어졌을 때, 다음 시점()의 상태와 보상이 발생할 확률.
* **우변:** 처음()부터 현재까지의 모든 역사(상태, 행동, 보상의 나열)가 주어졌을 때, 다음 시점의 상태와 보상이 발생할 확률.

이 등식이 성립한다면, 상태 는 **마르코프 성질**을 가진다고 말합니다.

### 2. 의미와 해석

**1) 과거의 정보는 불필요하다 (Independence of Past)**
마르코프 성질이 성립하면, 에이전트가 현재 어떤 경로를 거쳐서 상태 에 도달했는지는 중요하지 않습니다. 오직 '지금 어디에 있는지()'만이 다음 상태 $S_{t+1}$로 가는 확률 분포에 영향을 미칩니다.

**2) 상태의 충분성 (State Sufficiency)**
상태 는 미래를 결정짓는 데 필요한 과거의 모든 유용한 정보를 압축하여 담고 있어야 합니다. 만약 현재 정보만으로 미래를 예측하기 부족하다면(즉, 과거 정보를 더 알아야만 예측이 정확해진다면), 그 상태 정의는 마르코프 성질을 만족하지 못하는 것입니다.

### 3. 예시: 날아가는 공

이해를 돕기 위해 '공이 날아가는 상황'을 예로 들어보겠습니다.

* **마르코프 성질을 만족하지 못하는 상태 정의:**
* 상태정보: **[공의 현재 위치]**
* 이유: 공의 위치만 알면 이 공이 위로 올라가는 중인지, 떨어지는 중인지 알 수 없습니다. 다음 위치를 예측하려면 '직전 위치(과거)'를 알아야 속도를 추정할 수 있습니다. 따라서 현재 정보만으로는 미래를 완벽히 예측할 수 없습니다.


* **마르코프 성질을 만족하는 상태 정의:**
* 상태정보: **[공의 현재 위치, 공의 현재 속도]**
* 이유: 위치와 속도를 모두 안다면, 과거에 공을 누가 던졌는지, 어떤 궤적으로 날아왔는지 알 필요 없이 물리 법칙에 의해 다음 위치를 정확히 확률적으로 계산할 수 있습니다. 이것이 바로 마르코프 상태입니다.



### 4. 강화 학습에서의 중요성

강화 학습에서 환경을 MDP로 모델링한다는 것은 환경이 마르코프 성질을 따른다고 가정하는 것입니다.

1. **계산 복잡도 감소:** 과거의 모든 데이터를 기억하고 처리할 필요 없이, 현재 상태 만 보고 의사결정(Policy, )을 하면 되므로 문제가 훨씬 단순해집니다.
2. **이론적 기반:** 앞서 보여주신 이미지의 수식 (3.6)과 같은 **전이 확률(Transition Probability)** $p(s', r | s, a)$를 정의할 수 있게 되며, 이는 벨만 방정식(Bellman Equation) 등을 통해 최적의 행동을 찾는 수학적 기반이 됩니다.


