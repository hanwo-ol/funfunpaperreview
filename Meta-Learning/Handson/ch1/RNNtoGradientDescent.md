# ğŸ§  Learning to Learn by Gradient Descent by Gradient Descent

ì´ ë¬¸ì„œëŠ” Andrychowicz et al. (2016)ì˜ ë…¼ë¬¸ *â€œLearning to Learn by Gradient Descent by Gradient Descentâ€* ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ  
RNN ê¸°ë°˜ ë©”íƒ€ ì˜µí‹°ë§ˆì´ì €(meta-optimizer)ì˜ êµ¬ì¡°ì™€ ìˆ˜ì‹ì  í•´ì„ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.

---

## 1ï¸âƒ£ ì˜µí‹°ë§ˆì´ì €ì˜ ëª©í‘œ

ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬(Optimizee)ì˜ íŒŒë¼ë¯¸í„°ë¥¼ $\theta$,  
RNN ì˜µí‹°ë§ˆì´ì €ì˜ íŒŒë¼ë¯¸í„°ë¥¼ $\phi$ë¼ê³  í•˜ì.  

ë©”íƒ€ ì˜µí‹°ë§ˆì´ì €ì˜ ëª©í‘œëŠ” ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ì˜ í‰ê·  ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.

$$
L(\phi) = \mathbb{E}_f [f(\theta(f, \phi))]
$$

- $f$: base network (optimizee)ì˜ ì†ì‹¤ í•¨ìˆ˜  
- $\theta(f, \phi)$: RNN ì˜µí‹°ë§ˆì´ì €ì— ì˜í•´ ì—…ë°ì´íŠ¸ëœ íŒŒë¼ë¯¸í„°  
- $L(\phi)$: ì—¬ëŸ¬ íƒœìŠ¤í¬ì— ëŒ€í•œ ê¸°ëŒ€ ì†ì‹¤  

ì¦‰, RNNì˜ ì—­í• ì€ base networkì˜ ì†ì‹¤ì„ ì¤„ì´ëŠ” í•™ìŠµ ê·œì¹™ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.

---

## 2ï¸âƒ£ RNN ì˜µí‹°ë§ˆì´ì €ì˜ ì…ë ¥ê³¼ ì¶œë ¥

RNN ì˜µí‹°ë§ˆì´ì €ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤:

$$
(g_t, h_{t+1}) = m(\nabla_t, h_t, \phi)
$$

| ê¸°í˜¸ | ì˜ë¯¸ |
|------|------|
| $\nabla_t = \nabla_\theta f(\theta_t)$ | Base networkì˜ gradient (ì†ì‹¤ì˜ ê¸°ìš¸ê¸°) |
| $h_t$ | RNNì˜ hidden state (ì´ì „ ë‹¨ê³„ ì •ë³´ ì €ì¥) |
| $\phi$ | RNN ì˜µí‹°ë§ˆì´ì €ì˜ íŒŒë¼ë¯¸í„° |
| $g_t$ | RNNì˜ ì¶œë ¥ (ì—…ë°ì´íŠ¸ ë²¡í„°) |
| $h_{t+1}$ | RNNì˜ ë‹¤ìŒ hidden state |

ì¦‰, RNNì€ gradientì™€ ë‚´ë¶€ ìƒíƒœë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„,  
ë‹¤ìŒ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë°©í–¥ $g_t$ë¥¼ í•™ìŠµëœ ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡í•œë‹¤.

---

## 3ï¸âƒ£ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê·œì¹™

Base networkì˜ íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê°±ì‹ ëœë‹¤:

$$
\theta_{t+1} = \theta_t + g_t
$$

ì—¬ê¸°ì„œ $g_t$ëŠ” ì¼ë°˜ì ì¸ ê²½ì‚¬ í•˜ê°•ë²•ì˜ $-\alpha \nabla_\theta L$ ëŒ€ì‹   
RNNì´ ì¶œë ¥í•œ í•™ìŠµëœ ì—…ë°ì´íŠ¸ ë²¡í„°ì´ë‹¤.  

ì¦‰, ì—…ë°ì´íŠ¸ ê·œì¹™ì´ ê³ ì •ëœ ìˆ˜ì‹ì´ ì•„ë‹ˆë¼ í•™ìŠµëœ í•¨ìˆ˜ í˜•íƒœë¡œ í‘œí˜„ëœë‹¤.

---

## 4ï¸âƒ£ ë©”íƒ€ í•™ìŠµ ê³¼ì • (Meta-Optimization Loop)

ì „ì²´ í•™ìŠµì€ ë‘ ê°œì˜ ë£¨í”„ë¡œ êµ¬ì„±ëœë‹¤.

| ë‹¨ê³„ | ì„¤ëª… |
|------|------|
| Inner Loop | RNN ì˜µí‹°ë§ˆì´ì € $m_\phi$ë¥¼ ì´ìš©í•´ base network $f_\theta$ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ |
| Outer Loop | Inner loop ìˆ˜í–‰ í›„, ìµœì¢… ì†ì‹¤ì„ ê¸°ë°˜ìœ¼ë¡œ RNNì˜ íŒŒë¼ë¯¸í„° $\phi$ë¥¼ gradient descentë¡œ ì—…ë°ì´íŠ¸ |

ì¦‰, RNNì€ gradientë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì—…ë°ì´íŠ¸ ê·œì¹™ì„ í•™ìŠµí•˜ë©°,  
ê·¸ RNN ìì²´ëŠ” gradient descentë¡œ í•™ìŠµëœë‹¤.

---

## 5ï¸âƒ£ í•µì‹¬ ìš”ì•½

- RNNì€ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì™„ì „íˆ ëŒ€ì²´í•˜ì§€ ì•ŠëŠ”ë‹¤.  
  ëŒ€ì‹  gradientì™€ ë‚´ë¶€ stateë¥¼ í•¨ê»˜ ì…ë ¥ë°›ì•„, í•™ìŠµëœ ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ë¥¼ êµ¬ì„±í•œë‹¤.

- íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
\theta_{t+1} = \theta_t + g_t, \quad (g_t, h_{t+1}) = m(\nabla_t, h_t, \phi)
$$

- Outer loopì—ì„œ $\phi$ë¥¼ gradient descentë¡œ í•™ìŠµì‹œí‚¤ë¯€ë¡œ,  
  ì´ ê³¼ì •ì„ â€œê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ ê²½ì‚¬ í•˜ê°•ë²•ì„ í•™ìŠµí•œë‹¤â€ë¼ê³  ë¶€ë¥¸ë‹¤.

---

## 6ï¸âƒ£ í•¨ìˆ˜ $m$ì˜ ì •ì˜ì™€ ìˆ˜ì‹ì  êµ¬ì¡°

ìœ„ ì‹ì—ì„œ $m$ì€ ë‹¨ìˆœí•œ ê¸°í˜¸ê°€ ì•„ë‹ˆë¼, RNN ê¸°ë°˜ ì˜µí‹°ë§ˆì´ì €ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°í™”ëœ í•¨ìˆ˜ì´ë‹¤.  
ì¦‰, $m$ì€ ëª¨ë¸(model)ì´ì, ì—…ë°ì´íŠ¸ ê·œì¹™ì„ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜ë¡œ ì •ì˜ëœë‹¤.

$$
m_\phi : (\nabla_t, h_t) \mapsto (g_t, h_{t+1})
$$

- ì…ë ¥(Input):  
  - $\nabla_t$: optimizeeì˜ gradient  
  - $h_t$: ì´ì „ RNN hidden state  
- ì¶œë ¥(Output):  
  - $g_t$: íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë²¡í„°  
  - $h_{t+1}$: ë‹¤ìŒ hidden state  
- íŒŒë¼ë¯¸í„°(Parameter):  
  - $\phi$: RNN ì˜µí‹°ë§ˆì´ì €ì˜ ê°€ì¤‘ì¹˜ (ë©”íƒ€ ìˆ˜ì¤€ì—ì„œ í•™ìŠµë¨)

---

### ğŸ’¡ LSTM ê¸°ë°˜ êµ¬í˜„ ì˜ˆì‹œ

ì‹¤ì œë¡œëŠ” $m_\phi$ë¥¼ LSTMì´ë‚˜ GRU ì…€ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.  
ì˜ˆë¥¼ ë“¤ì–´, LSTM í˜•íƒœë¼ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë‚´ë¶€ ì—°ì‚°ìœ¼ë¡œ í‘œí˜„ëœë‹¤:

$$
\begin{aligned}
i_t &= \sigma(W_i \nabla_t + U_i h_t + b_i) \\
f_t &= \sigma(W_f \nabla_t + U_f h_t + b_f) \\
o_t &= \sigma(W_o \nabla_t + U_o h_t + b_o) \\
\tilde{c}_t &= \tanh(W_c \nabla_t + U_c h_t + b_c) \\
c_{t+1} &= f_t \odot c_t + i_t \odot \tilde{c}_t \\
h_{t+1} &= o_t \odot \tanh(c_{t+1}) \\
g_t &= W_g h_{t+1} + b_g
\end{aligned}
$$

ì´ë•Œ,  
- $(c_t, h_t)$ëŠ” RNNì˜ ë‚´ë¶€ ìƒíƒœ(state),  
- $g_t$ëŠ” base networkì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë²¡í„°ì´ë‹¤.  

ì¦‰, $m_\phi$ëŠ” â€œgradientë¥¼ ì—…ë°ì´íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•™ìŠµëœ RNN í•¨ìˆ˜â€ë¡œì„œ ë™ì‘í•œë‹¤.

---

## ğŸ§© ê²°ë¡ 

> RNN ì˜µí‹°ë§ˆì´ì € $m_\phi$ëŠ” gradient descentì˜ ê³ ì •ëœ ê·œì¹™ì„ í•™ìŠµ ê°€ëŠ¥í•œ í•¨ìˆ˜ë¡œ ì¼ë°˜í™”í•œ ê²ƒì´ë‹¤.  
>  
> ì´ í•¨ìˆ˜ëŠ” gradientì™€ ë‚´ë¶€ ìƒíƒœë¥¼ ì…ë ¥ë°›ì•„,  
> í•™ìŠµëœ ë°©ì‹ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë²¡í„°ë¥¼ ìƒì„±í•˜ë©°,  
> ë©”íƒ€ ìˆ˜ì¤€ì—ì„œëŠ” gradient descentë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµëœë‹¤.  
>  
> ë”°ë¼ì„œ, ì „ì²´ êµ¬ì¡°ëŠ” â€œlearning to learn by gradient descent by gradient descentâ€ë¡œ ìš”ì•½ëœë‹¤.
