### **siamese network의 아키텍처**

이제 **siamese network**에 대한 기본적인 이해를 갖추었으니, 이를 자세히 살펴보겠습니다. **siamese network**의 아키텍처는 다음 다이어그램과 같습니다:

<img width="548" height="505" alt="image" src="https://github.com/user-attachments/assets/5b3f3195-2981-45db-b6a5-86da1d4904f9" />

이미지 출처: https://learning.oreilly.com/library/view/hands-on-meta-learning/9781789534207/6e6eef6a-71fb-439e-a86d-f1e6ae88d258.xhtml

앞선 다이어그램에서 볼 수 있듯이, **siamese network**는 동일한 가중치와 아키텍처를 공유하는 두 개의 동일한 네트워크로 구성됩니다. 두 개의 입력 $X_1$과 $X_2$가 있다고 가정합시다. 입력 $X_1$을 네트워크 A에, 즉 $f_w(X_1)$에 입력하고, 입력 $X_2$를 네트워크 B에, 즉 $f_w(X_2)$에 입력합니다. 아시다시피, 이 두 네트워크는 동일한 가중치 $w$를 가지며, 입력 $X_1$과 $X_2$에 대한 임베딩을 생성합니다. 그런 다음, 이 임베딩들을 에너지 함수 E에 입력하면, 두 입력 간의 유사성을 얻게 됩니다.

다음과 같이 표현할 수 있습니다:

$$E_w(X_1, X_2) = \Vert f_w(X_1) - f_w(X_2) \Vert $$

에너지 함수로 유클리드 거리를 사용한다고 가정하면, $X_1$과 $X_2$가 유사할수록 E의 값은 작아집니다. 입력값이 다를수록 E의 값은 커집니다.

두 개의 문장, 문장 1과 문장 2가 있다고 가정해 봅시다. 문장 1을 네트워크 A에, 문장 2를 네트워크 B에 입력합니다. 네트워크 A와 네트워크 B가 모두 LSTM 네트워크이고 동일한 가중치를 공유한다고 가정합시다. 그러면 네트워크 A와 네트워크 B는 각각 문장 1과 문장 2에 대한 단어 임베딩을 생성합니다. 그런 다음 이 임베딩들을 에너지 함수에 입력하여 두 문장 간의 유사도 점수를 얻습니다. 하지만 **siamese network**를 어떻게 훈련시킬 수 있을까요? 데이터는 어떤 형태여야 할까요? 특징과 레이블은 무엇일까요? 우리의 목적 함수는 무엇일까요?

**siamese network**에 대한 입력은 쌍, 즉 $(X_1, X_2)$ 형태여야 하며, 입력 쌍이 진짜 쌍(동일)인지 가짜 쌍(다름)인지를 나타내는 이진 레이블 $Y \in \{0, 1\}$과 함께 제공되어야 합니다. 다음 표에서 볼 수 있듯이, 문장을 쌍으로 가지고 있으며 레이블은 문장 쌍이 진짜(1)인지 가짜(0)인지를 의미합니다:

| 문장 쌍 | 레이블 |
| :--- | :--- |
| 그녀는 아름다운 소녀입니다 (She is a beautiful girl) | 그녀는 아주 멋진 소녀입니다 (She is a gorgeous girl) | 1 |
| 새들이 하늘을 난다 (Birds fly in the sky) | 무엇을 하고 있니? (What are you doing?) | 0 |
| 나는 파리를 사랑해 (I love Paris) | 나는 파리를 정말 좋아해 (I adore Paris) | 1 |
| 그는 방금 도착했다 (He just arrived) | 나는 영화를 보고 있다 (I am watching a movie) | 0 |

그렇다면 우리 **siamese network**의 손실 함수는 무엇일까요? **siamese network**의 목표는 분류 작업을 수행하는 것이 아니라 두 입력값 간의 유사성을 이해하는 것이므로, 우리는 대조 손실 함수(contrastive loss function)를 사용합니다.

다음과 같이 표현할 수 있습니다:

$$\text{Contrastive Loss} = Y(E)^2 + (1 - Y)\max(\text{margin} - E, 0)^2$$

위 방정식에서 $Y$의 값은 실제 레이블로, 두 입력값이 유사하면 **1**이 되고, 다르면 **0**이 됩니다. 그리고 $E$는 우리의 에너지 함수로, 어떤 거리 측정 지표든 될 수 있습니다. `margin` 항은 제약 조건을 유지하는 데 사용됩니다. 즉, 두 입력값이 다르고 그 거리가 `margin`보다 크면 손실이 발생하지 않습니다.


---

## 가중치 $w$를 동일하게 가진다는 것의 의미를 잘 모르겠는데...

### 1. 가중치 $w$를 동일하게 가진다는 의미

이것은 개념적으로는 두 개의 네트워크(Network A, Network B)가 있는 것처럼 보이지만, 실제로는 **단 하나의 신경망**이 존재한다는 의미입니다.

*   **물리적 실체**: 코드 상에서는 신경망 모델을 **하나만** 정의합니다.
*   **두 번의 사용**: 입력 $X_1$을 그 모델에 넣어 결과 $f_w(X_1)$을 얻고, **바로 그 동일한 모델**에 입력 $X_2$를 넣어 결과 $f_w(X_2)$를 얻습니다.
*   **파라미터 공유(Parameter Sharing)**: "가중치를 공유한다"는 것은 이 개념을 기술적으로 표현하는 용어입니다. Network A의 첫 번째 레이어의 가중치 행렬은 Network B의 첫 번째 레이어 가중치 행렬과 완전히 동일합니다. 두 번째, 세 번째... 마지막 레이어까지 모든 파라미터(가중치와 편향)가 100% 동일합니다.

Network A와 Network B는 별개의 존재가 아니라, **동일한 하나의 신경망**을 가리키는 것입니다.



### 2. 가중치를 동일하게 가져야 하는 이유

결론부터 말하면, **두 입력을 '같은 기준'으로 비교하기 위해서**입니다. 만약 두 네트워크가 서로 다른 가중치($w_A$와 $w_B$)를 가진다면, 두 입력은 서로 다른 기준으로 평가되므로 그 결과를 비교하는 것이 의미가 없어집니다.

더 구체적인 이유는 다음과 같습니다.

#### **1) 동일한 특징 공간(Feature Space)으로의 매핑**

Siamese Network의 목표는 입력 데이터(이미지, 문장 등)를 저차원의 벡터 공간(특징 공간 또는 임베딩 공간)으로 매핑하는 함수 $f_w(\cdot)$를 학습하는 것입니다. 이 공간에서는 **의미가 비슷한 입력은 가까운 곳에, 의미가 다른 입력은 먼 곳에 위치**해야 합니다.

*   만약 가중치가 같다면 ($w_A = w_B = w$), $X_1$과 $X_2$는 **동일한 규칙**에 따라 **하나의 일관된 특징 공간**으로 매핑됩니다. 그래야만 그 공간 안에서 두 벡터 $f_w(X_1)$과 $f_w(X_2)$ 사이의 거리를 재는 것이 의미가 있습니다.

*   만약 가중치가 다르다면 ($w_A \neq w_B$), $X_1$은 $f_{w_A}$라는 규칙에 따라 '공간 A'로 매핑되고, $X_2$는 $f_{w_B}$라는 다른 규칙에 따라 '공간 B'로 매핑됩니다. **서로 다른 두 공간에 있는 벡터들의 거리를 재는 것은 사과와 오렌지를 비교하는 것처럼 의미가 없습니다.**

#### **2) 대칭성(Symmetry) 보장**

두 입력의 유사도는 대칭적이어야 합니다. 즉, `유사도(A, B)`는 `유사도(B, A)`와 같아야 합니다.

*   가중치를 공유하면, 입력 순서를 바꿔도 최종 결과(두 벡터 사이의 거리)는 항상 동일합니다.

$$\Vert f_w(X_1) - f_w(X_2) \Vert = \Vert f_w(X_2) - f_w(X_1) \Vert $$

*   만약 가중치가 다르다면, 대칭성이 보장되지 않아 입력 순서에 따라 결과가 달라질 수 있습니다.

따라서 Siamese Network에서 가중치를 공유하는 것은 선택 사항이 아니라, **두 입력을 공정하고 의미 있게 비교하기 위한 필수적인 설계 원칙**입니다.


---

## loss function의 의미 더 자세하게 파헤쳐보자...

### 대조 손실 함수(Contrastive Loss)의 전체 목표

이 함수의 궁극적인 목표는 **"의미가 비슷한 것들은 가깝게, 의미가 다른 것들은 멀게"** 만드는 것입니다. 이를 위해 입력으로 들어온 한 쌍의 데이터 $(X_1, X_2)$가 비슷한지(`Y=1`), 다른지(`Y=0`)에 따라 네트워크에 다른 종류의 "벌칙(penalty)"을 부여합니다.

- **`Y`**: 두 입력 $X_1, X_2$의 실제 관계를 나타내는 레이블입니다.
- **`E`**: 에너지 함수(Energy function)로, 두 입력의 임베딩 벡터 간의 **거리(distance)**를 의미합니다. 즉, $E = \| f_w(X_1) - f_w(X_2) \|$ 입니다. `E`가 작을수록 가깝고, 클수록 멉니다.
- **`margin`**: 하이퍼파라미터로, "의미가 다른 데이터 쌍은 **최소한 이 거리만큼은 떨어져 있어야 한다**"고 정해주는 기준선입니다.

### Case 1: 두 입력이 유사할 때 (Similar Pair, Y=1)

> Question 1
> 
> X_1, X_2가 실제로는 similar일 때 Y=1이 되는거지? 그럼 Y(E)^2를 줄이는 것의 의미는 뭐야?

두 이미지가 같은 사람의 얼굴이거나, 두 문장의 의미가 같을 때 **`Y=1`**이 됩니다.

이 경우, 손실 함수는 다음과 같이 간단해집니다.

$$\text{Loss} = 1 \cdot (E)^2 + (1 - 1) \cdot \max(\text{margin} - E, 0)^2$$

$$\text{Loss} = (E)^2$$

**의미**: 손실 함수는 단순히 **거리의 제곱($E^2$)**이 됩니다. 머신러닝 모델은 항상 손실(Loss)을 최소화하는 방향으로 학습합니다. 따라서 `Loss`, 즉 $E^2$을 줄이는 것은 **두 임베딩 벡터 사이의 거리 `E`를 줄이는 것**을 의미합니다.

- **직관적인 해석**: "이 두 입력은 서로 비슷하니까, 임베딩 공간에서 둘의 위치를 최대한 가깝게 만들어라! 이상적으로는 거리가 0이 되도록 해라."
- **학습 과정**: 네트워크는 두 입력 $X_1$과 $X_2$를 임베딩 공간의 거의 같은 지점으로 매핑하도록 가중치($w$)를 업데이트합니다.

---

### Case 2: 두 입력이 다를 때 (Dissimilar Pair, Y=0)

> Question 2
>
> X_1, X_2가 실제로는 dissimilar일 때 Y=0이 되는거지? 그럼 (1 - Y)\max(\text{margin} - E, 0)^2항을 줄이게 될텐데, \max(\text{margin} - E, 0)^2의 의미?

두 이미지가 다른 사람의 얼굴이거나, 두 문장의 의미가 다를 때 **`Y=0`**이 됩니다.

이 경우, 손실 함수는 다음과 같이 변합니다.

$$\text{Loss} = 0 \cdot (E)^2 + (1 - 0) \cdot \max(\text{margin} - E, 0)^2$$

$$\text{Loss} = \max(\text{margin} - E, 0)^2$$

이것이 바로 질문의 핵심입니다. **`max(margin - E, 0)`**의 의미를 이해하기 위해 두 가지 상황으로 나누어 보겠습니다.

#### 상황 A: 두 벡터의 거리가 `margin`보다 가까울 때 ($E < \text{margin}$)

- 두 입력이 서로 다른데도 불구하고, 임베딩 공간에서 너무 가깝게 위치해 있습니다. (예: `margin`=2인데, `E`=0.5)
- 이때 `margin - E`는 양수 값(e.g., 2 - 0.5 = 1.5)이 됩니다.
- `max(margin - E, 0)`은 `margin - E`가 됩니다.
- 최종 Loss는 `(margin - E)^2`이 되어 0보다 큰 양수 값을 갖습니다.
- **의미**: "이 두 입력은 서로 다른데 왜 이렇게 가까이 붙어있어? **벌칙이야!** 둘 사이의 거리 `E`를 `margin`보다 멀어질 때까지 늘려라!"
- **학습 과정**: 네트워크는 손실을 줄이기 위해 `E` 값을 키우는 방향으로, 즉 두 임베딩 벡터를 서로 밀어내는 방향으로 가중치를 업데이트합니다.

#### 상황 B: 두 벡터의 거리가 `margin`보다 멀거나 같을 때 ($E \ge \text{margin}$)

- 두 입력이 서로 다르며, 임베딩 공간에서도 이미 충분히 멀리 떨어져 있습니다. (예: `margin`=2인데, `E`=3)
- 이때 `margin - E`는 0 또는 음수 값(e.g., 2 - 3 = -1)이 됩니다.
- **`max(margin - E, 0)`은 `0`이 됩니다.**
- 최종 Loss는 `0^2 = 0`이 됩니다.
- **의미**: "이 두 입력은 서로 다른데, 이미 충분히(`margin`만큼) 떨어져 있구나. **잘했어, 벌칙 없어!** 더 이상 밀어내려고 애쓰지 않아도 돼."
- **학습 과정**: 손실이 0이므로, 이 데이터 쌍에 대해서는 네트워크의 가중치가 업데이트되지 않습니다. 이는 매우 효율적인 방식입니다. 모델이 이미 잘하고 있는 부분에 대해서는 에너지를 낭비하지 않도록 합니다.

### 요약

| 데이터 쌍 종류 | 레이블 (Y) | 활성화되는 Loss 항 | 목표 | 직관적인 행동 |
| :--- | :--- | :--- | :--- | :--- |
| **유사한 쌍** | 1 | $E^2$ | 거리 `E`를 **최소화** (0에 가깝게) | 두 개의 자석을 서로 **끌어당김** |
| **다른 쌍** | 0 | $\max(\text{margin} - E, 0)^2$ | 거리 `E`가 `margin`보다 **커지도록** | 두 개의 자석을 서로 **밀어내되**, `margin` 거리까지만 밀어냄 |

결론적으로, 대조 손실 함수는 **유사한 쌍은 한없이 가깝게 끌어당기고, 다른 쌍은 최소한 `margin`이라는 안전거리 확보될 때까지만 밀어내는** 영리한 방식의 손실 함수입니다. 이를 통해 전체 임베딩 공간이 의미적으로 잘 구조화되도록 유도합니다.
