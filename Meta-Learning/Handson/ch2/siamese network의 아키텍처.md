### **siamese network의 아키텍처**

이제 **siamese network**에 대한 기본적인 이해를 갖추었으니, 이를 자세히 살펴보겠습니다. **siamese network**의 아키텍처는 다음 다이어그램과 같습니다:

<img width="548" height="505" alt="image" src="https://github.com/user-attachments/assets/5b3f3195-2981-45db-b6a5-86da1d4904f9" />

이미지 출처: https://learning.oreilly.com/library/view/hands-on-meta-learning/9781789534207/6e6eef6a-71fb-439e-a86d-f1e6ae88d258.xhtml

앞선 다이어그램에서 볼 수 있듯이, **siamese network**는 동일한 가중치와 아키텍처를 공유하는 두 개의 동일한 네트워크로 구성됩니다. 두 개의 입력 $X_1$과 $X_2$가 있다고 가정합시다. 입력 $X_1$을 네트워크 A에, 즉 $f_w(X_1)$에 입력하고, 입력 $X_2$를 네트워크 B에, 즉 $f_w(X_2)$에 입력합니다. 아시다시피, 이 두 네트워크는 동일한 가중치 $w$를 가지며, 입력 $X_1$과 $X_2$에 대한 임베딩을 생성합니다. 그런 다음, 이 임베딩들을 에너지 함수 E에 입력하면, 두 입력 간의 유사성을 얻게 됩니다.

다음과 같이 표현할 수 있습니다:

$$E_w(X_1, X_2) = \Vert f_w(X_1) - f_w(X_2) \Vert $$

에너지 함수로 유클리드 거리를 사용한다고 가정하면, $X_1$과 $X_2$가 유사할수록 E의 값은 작아집니다. 입력값이 다를수록 E의 값은 커집니다.

두 개의 문장, 문장 1과 문장 2가 있다고 가정해 봅시다. 문장 1을 네트워크 A에, 문장 2를 네트워크 B에 입력합니다. 네트워크 A와 네트워크 B가 모두 LSTM 네트워크이고 동일한 가중치를 공유한다고 가정합시다. 그러면 네트워크 A와 네트워크 B는 각각 문장 1과 문장 2에 대한 단어 임베딩을 생성합니다. 그런 다음 이 임베딩들을 에너지 함수에 입력하여 두 문장 간의 유사도 점수를 얻습니다. 하지만 **siamese network**를 어떻게 훈련시킬 수 있을까요? 데이터는 어떤 형태여야 할까요? 특징과 레이블은 무엇일까요? 우리의 목적 함수는 무엇일까요?

**siamese network**에 대한 입력은 쌍, 즉 $(X_1, X_2)$ 형태여야 하며, 입력 쌍이 진짜 쌍(동일)인지 가짜 쌍(다름)인지를 나타내는 이진 레이블 $Y \in \{0, 1\}$과 함께 제공되어야 합니다. 다음 표에서 볼 수 있듯이, 문장을 쌍으로 가지고 있으며 레이블은 문장 쌍이 진짜(1)인지 가짜(0)인지를 의미합니다:

| 문장 쌍 | 레이블 |
| :--- | :--- |
| 그녀는 아름다운 소녀입니다 (She is a beautiful girl) | 그녀는 아주 멋진 소녀입니다 (She is a gorgeous girl) | 1 |
| 새들이 하늘을 난다 (Birds fly in the sky) | 무엇을 하고 있니? (What are you doing?) | 0 |
| 나는 파리를 사랑해 (I love Paris) | 나는 파리를 정말 좋아해 (I adore Paris) | 1 |
| 그는 방금 도착했다 (He just arrived) | 나는 영화를 보고 있다 (I am watching a movie) | 0 |

그렇다면 우리 **siamese network**의 손실 함수는 무엇일까요? **siamese network**의 목표는 분류 작업을 수행하는 것이 아니라 두 입력값 간의 유사성을 이해하는 것이므로, 우리는 대조 손실 함수(contrastive loss function)를 사용합니다.

다음과 같이 표현할 수 있습니다:

$\text{Contrastive Loss} = Y(E)^2 + (1 - Y)\max(\text{margin} - E, 0)^2$

위 방정식에서 $Y$의 값은 실제 레이블로, 두 입력값이 유사하면 **1**이 되고, 다르면 **0**이 됩니다. 그리고 $E$는 우리의 에너지 함수로, 어떤 거리 측정 지표든 될 수 있습니다. `margin` 항은 제약 조건을 유지하는 데 사용됩니다. 즉, 두 입력값이 다르고 그 거리가 `margin`보다 크면 손실이 발생하지 않습니다.


---

## 가중치 $w$를 동일하게 가진다는 것의 의미를 잘 모르겠는데...

### 1. 가중치 $w$를 동일하게 가진다는 의미

이것은 개념적으로는 두 개의 네트워크(Network A, Network B)가 있는 것처럼 보이지만, 실제로는 **단 하나의 신경망**이 존재한다는 의미입니다.

*   **물리적 실체**: 코드 상에서는 신경망 모델을 **하나만** 정의합니다.
*   **두 번의 사용**: 입력 $X_1$을 그 모델에 넣어 결과 $f_w(X_1)$을 얻고, **바로 그 동일한 모델**에 입력 $X_2$를 넣어 결과 $f_w(X_2)$를 얻습니다.
*   **파라미터 공유(Parameter Sharing)**: "가중치를 공유한다"는 것은 이 개념을 기술적으로 표현하는 용어입니다. Network A의 첫 번째 레이어의 가중치 행렬은 Network B의 첫 번째 레이어 가중치 행렬과 완전히 동일합니다. 두 번째, 세 번째... 마지막 레이어까지 모든 파라미터(가중치와 편향)가 100% 동일합니다.

Network A와 Network B는 별개의 존재가 아니라, **동일한 하나의 신경망**을 가리키는 것입니다.



### 2. 가중치를 동일하게 가져야 하는 이유

결론부터 말하면, **두 입력을 '같은 기준'으로 비교하기 위해서**입니다. 만약 두 네트워크가 서로 다른 가중치($w_A$와 $w_B$)를 가진다면, 두 입력은 서로 다른 기준으로 평가되므로 그 결과를 비교하는 것이 의미가 없어집니다.

더 구체적인 이유는 다음과 같습니다.

#### **1) 동일한 특징 공간(Feature Space)으로의 매핑**

Siamese Network의 목표는 입력 데이터(이미지, 문장 등)를 저차원의 벡터 공간(특징 공간 또는 임베딩 공간)으로 매핑하는 함수 $f_w(\cdot)$를 학습하는 것입니다. 이 공간에서는 **의미가 비슷한 입력은 가까운 곳에, 의미가 다른 입력은 먼 곳에 위치**해야 합니다.

*   만약 가중치가 같다면 ($w_A = w_B = w$), $X_1$과 $X_2$는 **동일한 규칙**에 따라 **하나의 일관된 특징 공간**으로 매핑됩니다. 그래야만 그 공간 안에서 두 벡터 $f_w(X_1)$과 $f_w(X_2)$ 사이의 거리를 재는 것이 의미가 있습니다.

*   만약 가중치가 다르다면 ($w_A \neq w_B$), $X_1$은 $f_{w_A}$라는 규칙에 따라 '공간 A'로 매핑되고, $X_2$는 $f_{w_B}$라는 다른 규칙에 따라 '공간 B'로 매핑됩니다. **서로 다른 두 공간에 있는 벡터들의 거리를 재는 것은 사과와 오렌지를 비교하는 것처럼 의미가 없습니다.**

#### **2) 대칭성(Symmetry) 보장**

두 입력의 유사도는 대칭적이어야 합니다. 즉, `유사도(A, B)`는 `유사도(B, A)`와 같아야 합니다.

*   가중치를 공유하면, 입력 순서를 바꿔도 최종 결과(두 벡터 사이의 거리)는 항상 동일합니다.

$$\Vert f_w(X_1) - f_w(X_2) \Vert = \Vert f_w(X_2) - f_w(X_1) \Vert $$

*   만약 가중치가 다르다면, 대칭성이 보장되지 않아 입력 순서에 따라 결과가 달라질 수 있습니다.

따라서 Siamese Network에서 가중치를 공유하는 것은 선택 사항이 아니라, **두 입력을 공정하고 의미 있게 비교하기 위한 필수적인 설계 원칙**입니다.
