# ğŸ§  Learning to Learn by Gradient Descent by Gradient Descent

ì´ ë¬¸ì„œëŠ” Andrychowicz et al. (2016)ì˜ ë…¼ë¬¸ *â€œLearning to Learn by Gradient Descent by Gradient Descentâ€* ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ  
RNN ê¸°ë°˜ ë©”íƒ€ ì˜µí‹°ë§ˆì´ì €(meta-optimizer)ì˜ êµ¬ì¡°ì™€ ìˆ˜ì‹ì  í•´ì„ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.

---

## 1ï¸âƒ£ ì˜µí‹°ë§ˆì´ì €ì˜ ëª©í‘œ

ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬(Optimizee)ì˜ íŒŒë¼ë¯¸í„°ë¥¼ $\theta$,  
RNN ì˜µí‹°ë§ˆì´ì €ì˜ íŒŒë¼ë¯¸í„°ë¥¼ $\phi$ë¼ê³  í•˜ì.  

ë©”íƒ€ ì˜µí‹°ë§ˆì´ì €ì˜ ëª©í‘œëŠ” ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ì˜ í‰ê·  ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.

$$
L(\phi) = \mathbb{E} _f [f(\theta(f, \phi))]
$$

- $f$: base network (optimizee)ì˜ ì†ì‹¤ í•¨ìˆ˜  
- $\theta(f, \phi)$: RNN ì˜µí‹°ë§ˆì´ì €ì— ì˜í•´ ì—…ë°ì´íŠ¸ëœ íŒŒë¼ë¯¸í„°  
- $L(\phi)$: ì—¬ëŸ¬ íƒœìŠ¤í¬ì— ëŒ€í•œ ê¸°ëŒ€ ì†ì‹¤  

ì¦‰, RNNì˜ ì—­í• ì€ base networkì˜ ì†ì‹¤ì„ ì¤„ì´ëŠ” í•™ìŠµ ê·œì¹™ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.

---

## 2ï¸âƒ£ RNN ì˜µí‹°ë§ˆì´ì €ì˜ ì…ë ¥ê³¼ ì¶œë ¥

RNN ì˜µí‹°ë§ˆì´ì €ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤:

$$
(g _t, h _{t+1}) = m(\nabla _t, h _t, \phi)
$$

| ê¸°í˜¸ | ì˜ë¯¸ |
|------|------|
| $\nabla _t = \nabla _\theta f(\theta _t)$ | Base networkì˜ gradient (ì†ì‹¤ì˜ ê¸°ìš¸ê¸°) |
| $h _t$ | RNNì˜ hidden state (ì´ì „ ë‹¨ê³„ ì •ë³´ ì €ì¥) |
| $\phi$ | RNN ì˜µí‹°ë§ˆì´ì €ì˜ íŒŒë¼ë¯¸í„° |
| $g _t$ | RNNì˜ ì¶œë ¥ (ì—…ë°ì´íŠ¸ ë²¡í„°) |
| $h _{t+1}$ | RNNì˜ ë‹¤ìŒ hidden state |

ì¦‰, RNNì€ gradientì™€ ë‚´ë¶€ ìƒíƒœë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„,  
ë‹¤ìŒ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë°©í–¥ $g _t$ë¥¼ í•™ìŠµëœ ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡í•œë‹¤.

---

## 3ï¸âƒ£ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê·œì¹™

Base networkì˜ íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê°±ì‹ ëœë‹¤:

$$
\theta _{t+1} = \theta _t + g _t
$$

ì—¬ê¸°ì„œ $g _t$ëŠ” ì¼ë°˜ì ì¸ ê²½ì‚¬ í•˜ê°•ë²•ì˜ $-\alpha \nabla _\theta L$ ëŒ€ì‹   
RNNì´ ì¶œë ¥í•œ í•™ìŠµëœ ì—…ë°ì´íŠ¸ ë²¡í„°ì´ë‹¤.  

ì¦‰, ì—…ë°ì´íŠ¸ ê·œì¹™ì´ ê³ ì •ëœ ìˆ˜ì‹ì´ ì•„ë‹ˆë¼ í•™ìŠµëœ í•¨ìˆ˜ í˜•íƒœë¡œ í‘œí˜„ëœë‹¤.

---

## 4ï¸âƒ£ ë©”íƒ€ í•™ìŠµ ê³¼ì • (Meta-Optimization Loop)

ì „ì²´ í•™ìŠµì€ ë‘ ê°œì˜ ë£¨í”„ë¡œ êµ¬ì„±ëœë‹¤.

| ë‹¨ê³„ | ì„¤ëª… |
|------|------|
| Inner Loop | RNN ì˜µí‹°ë§ˆì´ì € $m _\phi$ë¥¼ ì´ìš©í•´ base network $f _\theta$ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ |
| Outer Loop | Inner loop ìˆ˜í–‰ í›„, ìµœì¢… ì†ì‹¤ì„ ê¸°ë°˜ìœ¼ë¡œ RNNì˜ íŒŒë¼ë¯¸í„° $\phi$ë¥¼ gradient descentë¡œ ì—…ë°ì´íŠ¸ |

ì¦‰, RNNì€ gradientë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì—…ë°ì´íŠ¸ ê·œì¹™ì„ í•™ìŠµí•˜ë©°,  
ê·¸ RNN ìì²´ëŠ” gradient descentë¡œ í•™ìŠµëœë‹¤.

---

## 5ï¸âƒ£ í•µì‹¬ ìš”ì•½

- RNNì€ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì™„ì „íˆ ëŒ€ì²´í•˜ì§€ ì•ŠëŠ”ë‹¤.  
  ëŒ€ì‹  gradientì™€ ë‚´ë¶€ stateë¥¼ í•¨ê»˜ ì…ë ¥ë°›ì•„, í•™ìŠµëœ ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ë¥¼ êµ¬ì„±í•œë‹¤.

- íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
\theta _{t+1} = \theta _t + g _t, \quad (g _t, h _{t+1}) = m(\nabla _t, h _t, \phi)
$$

- Outer loopì—ì„œ $\phi$ë¥¼ gradient descentë¡œ í•™ìŠµì‹œí‚¤ë¯€ë¡œ,  
  ì´ ê³¼ì •ì„ â€œê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ ê²½ì‚¬ í•˜ê°•ë²•ì„ í•™ìŠµí•œë‹¤â€ë¼ê³  ë¶€ë¥¸ë‹¤.

---

> ìš”ì•½í•˜ìë©´:  
> - RNNì€ gradient descentì˜ ìˆ˜ì‹ì„ í•™ìŠµ ê°€ëŠ¥í•œ í•¨ìˆ˜ë¡œ ì¼ë°˜í™”í•œë‹¤.  
> - gradientì™€ hidden stateë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„,  
>   ê²½í—˜ì ìœ¼ë¡œ ë” ë‚˜ì€ ì—…ë°ì´íŠ¸ ê·œì¹™ì„ ìƒì„±í•œë‹¤.  
> - ê²°ê³¼ì ìœ¼ë¡œ, â€œlearning to learn by gradient descent by gradient descentâ€ê°€ ì‹¤í˜„ëœë‹¤.
