### **3.2.1 피어슨 및 우도비 카이제곱 검정 (Pearson and Likelihood-Ratio Chi-Squared Tests)**

**[원문 번역]**

1.5.2절에서 우리는 특정 값의 다항 확률에 대한 검정을 위해 피어슨 $X^2$ 통계량(1.16)을 소개했습니다. 독립성($H_0$) 검정은 $n_i$ 대신 $n_{ij}$를, $\mu_i$ 대신 $\mu_{ij} = n\pi_{i+}\pi_{+j}$를 사용하여 $X^2$을 적용합니다. 여기서 $\mu_{ij} = E(n_{ij})$는 $H_0$ 하에서의 기댓값입니다. 보통 $\{\pi_{i+}\}$와 $\{\pi_{+j}\}$는 알려져 있지 않습니다. 이들의 최대우도(ML) 추정치는 표본 주변 비율인 $\hat{\pi}_{i+} = n_{i+}/n$ 과 $\hat{\pi}_{+j} = n_{+j}/n$ 입니다. 따라서, 추정된 기대 빈도는 $\{\hat{\mu}_{ij} = n\hat{\pi}_{i+}\hat{\pi}_{+j} = n_{i+}n_{+j}/n\}$ 입니다. 그러면, 피어슨 통계량은 다음과 같습니다.

$$ X^2 = \sum_i \sum_j \frac{(n_{ij} - \hat{\mu}_{ij})^2}{\hat{\mu}_{ij}} \quad (3.10) $$

피어슨(1900, 1904, 1922)은 $\{\mu_{ij}\}$를 추정치 $\{\hat{\mu}_{ij}\}$로 대체하는 것이 $X^2$의 대푯본 분포에 영향을 미치지 않을 것이라고 주장했습니다. 분할표는 $IJ$개의 범주를 가지므로, 그는 $X^2$이 점근적으로 자유도 df = $IJ-1$인 카이제곱 분포를 따를 것이라고 주장했습니다. 반대로, $\{\hat{\mu}_{ij}\}$가 $\{\pi_{i+}\}$와 $\{\pi_{+j}\}$의 추정을 필요로 하기 때문에(1.5.6절에 의해),

$$ \text{df} = (IJ - 1) - (I-1) - (J-1) = (I-1)(J-1) $$

입니다. $\{\pi_{i+}\}$와 $\{\pi_{+j}\}$의 차원은 $\sum_i \pi_{i+} = \sum_j \pi_{+j} = 1$이라는 제약을 반영합니다. R. A. Fisher(1922)가 피어슨의 오류를 수정했습니다. 피셔의 논문은 **자유도(degrees of freedom)**라는 개념을 도입했습니다. (피어슨은 인덱스가 있는 카이제곱 분포 족을 도입했지만, "자유도"를 명시적으로 다루지는 않았습니다.)

스코어 검정은 $X^2$ 통계량을 생성합니다. 우도비 검정은 다른 통계량을 생성합니다. 다항 표집에서, 우도의 커널은 다음과 같습니다.

$$ \prod_i \prod_j \pi_{ij}^{n_{ij}}, \quad \text{여기서 모든 } \pi_{ij} \ge 0 \text{ 이고 } \sum_i \sum_j \pi_{ij} = 1 $$

$H_0$: 독립성 하에서, $\hat{\pi}_{ij} = \hat{\pi}_{i+}\hat{\pi}_{+j} = n_{i+}n_{+j}/n^2$ 입니다. 일반적인 경우, $\hat{\pi}_{ij} = n_{ij}/n$ 입니다. 우도들의 비율은 다음과 같습니다.

$$ \Lambda = \frac{\prod_i \prod_j (n_{i+}n_{+j}/n^2)^{n_{ij}}}{\prod_i \prod_j (n_{ij}/n)^{n_{ij}}} $$

우도비 카이제곱 통계량은 $-2 \log \Lambda$ 입니다. $G^2$으로 표기되며, 다음과 같습니다.

$$ G^2 = -2 \log \Lambda = 2 \sum_i \sum_j n_{ij} \log(n_{ij}/\hat{\mu}_{ij}) \quad (3.11) $$

$G^2$과 $X^2$의 값이 클수록 독립성에 반하는 증거가 더 많아집니다. 두 통계량 모두, P-값은 관측된 값보다 큰 오른쪽 꼬리 확률입니다.

일반적인 경우, 모수 공간은 선형 제약 $\sum_i\sum_j \pi_{ij} = 1$을 따르는 $\{\pi_{ij}\}$로 구성되므로, 차원은 $IJ-1$입니다. $H_0$ 하에서, $\{\pi_{ij}\}$는 $\{\pi_{i+}\}$와 $\{\pi_{+j}\}$에 의해 결정되므로, 차원은 $(I-1) + (J-1)$입니다. 이 차원들의 차이는 $(I-1)(J-1)$입니다. 대푯본에 대해, $G^2$은 df = $(I-1)(J-1)$인 카이제곱 귀무분포를 가집니다. 따라서 $G^2$과 $X^2$은 동일한 극한 귀무분포를 가집니다. 사실, 이 경우 둘은 점근적으로 동등합니다; $X^2-G^2$은 확률적으로 0으로 수렴합니다 (16.3.4절).

---

### **핵심 내용 해설: 두 변수는 서로 관련이 있는가?**

이 섹션은 분할표 분석의 가장 근본적인 질문, 즉 **"두 변수 사이에 연관성이 있는가, 아니면 서로 독립인가?"**에 답하기 위한 두 가지 표준적인 방법을 소개합니다.

#### **1. 독립성이라는 귀무가설($H_0$)**

*   **독립성의 정의**: 한 변수의 결과를 아는 것이 다른 변수의 결과를 예측하는 데 아무런 도움이 되지 않는 상태.
*   **수학적 표현**: $P(\text{행=i, 열=j}) = P(\text{행=i}) \times P(\text{열=j})$
    *   $\pi_{ij} = \pi_{i+} \times \pi_{+j}$
*   **기대 빈도($\mu_{ij}$)**: 만약 두 변수가 정말 독립이라면, $(i,j)$ 셀에서 기대할 수 있는 빈도의 수는 다음과 같습니다.
    *   $\mu_{ij} = n \times \pi_{ij} = n \times \pi_{i+} \times \pi_{+j}$

#### **2. 문제점: 실제 확률을 모른다**

멘델의 예제와 달리, 대부분의 실제 상황에서는 귀무가설 하의 주변 확률($\pi_{i+}, \pi_{+j}$)을 알지 못합니다.
*   **해결책**: 모르는 것은 데이터로부터 **추정**합니다.
    *   $\pi_{i+}$의 최선의 추정치: 행의 표본 비율 $\hat{\pi}_{i+} = n_{i+}/n$
    *   $\pi_{+j}$의 최선의 추정치: 열의 표본 비율 $\hat{\pi}_{+j} = n_{+j}/n$
*   **추정된 기대 빈도($\hat{\mu}_{ij}$)**: 이 추정치들을 사용하여 기대 빈도를 계산합니다.

$$ \hat{\mu}_{ij} = n \times \hat{\pi}_{i+} \times \hat{\pi}_{+j} = n \times \frac{n_{i+}}{n} \times \frac{n_{+j}}{n} = \frac{n_{i+}n_{+j}}{n} $$

이 공식은 "독립성 하에서 기대되는 빈도는 (해당 행의 합) $\times$ (해당 열의 합) / (전체 합) 이다"라는 매우 중요한 규칙을 보여줍니다.

#### **3. 피어슨 $X^2$ 통계량 (공식 3.10)**

*   **아이디어**: 1.5.2절과 동일합니다. 관측값과 기대값 사이의 거리를 측정합니다. 단, 이번에는 이론적인 기대값 $\mu_{ij}$ 대신 **데이터로부터 추정된 기대값 $\hat{\mu}_{ij}$**를 사용합니다.

$$ X^2 = \sum_i \sum_j \frac{(n_{ij} - \hat{\mu}_{ij})^2}{\hat{\mu}_{ij}} $$

*   이 통계량의 값이 클수록 관측된 데이터가 독립성 가정에서 멀리 벗어남을 의미합니다.

#### **4. 우도비 $G^2$ 통계량 (공식 3.11)**

*   **아이디어**: 1.5.3절과 동일합니다. "독립성이라는 제약이 있는 모델(축소 모델)"과 "제약이 없는 일반 모델(전체 모델)"의 설명력(우도)을 비교합니다.
    *   **전체 모델의 설명력**: 각 셀 확률을 표본 셀 비율($\hat{\pi}_{ij}=n_{ij}/n$)로 추정. 기대 빈도는 $n \times (n_{ij}/n) = n_{ij}$로, 관측 빈도 그 자체가 됩니다.
    *   **축소 모델(독립성)의 설명력**: 각 셀 확률을 주변 비율의 곱($\hat{\pi}_{i+}\hat{\pi}_{+j}$)으로 추정. 기대 빈도는 $\hat{\mu}_{ij}$가 됩니다.
*   이를 우도비 공식에 대입하여 정리하면 다음과 같습니다.

$$ G^2 = 2 \sum_i \sum_j n_{ij} \log\left(\frac{n_{ij}}{\hat{\mu}_{ij}}\right) = 2 \sum (\text{관측}) \log\left(\frac{\text{관측}}{\text{기대}}\right) $$

*   이 통계량의 값이 클수록 독립성 모델의 설명력이 상대적으로 나쁘다는 의미입니다.

#### **5. 자유도(Degrees of Freedom)의 비밀: 피셔의 위대한 수정**

*   **피어슨의 착각**: 피어슨은 기대 빈도를 데이터로부터 추정하더라도 자유도는 변하지 않고 $IJ-1$이라고 생각했습니다.
*   **피셔의 통찰**: 피셔는 **"데이터를 사용하여 모수를 추정할 때마다, 우리는 데이터에 제약을 가하는 것이며, 그 대가로 자유도를 1씩 잃는다"**는 것을 밝혔습니다.
    *   **총 정보량**: 분할표는 원래 $IJ-1$개의 자유로운 정보를 가집니다. (모든 셀 확률의 합은 1이므로)
    *   **소모된 정보량**: 독립성 모델의 기대 빈도를 계산하기 위해, 우리는 $I-1$개의 행 주변 확률과 $J-1$개의 열 주변 확률을 데이터로부터 추정해야 했습니다. (각각 합이 1이므로 하나씩 덜 추정)
    *   **남은 정보량 (자유도)**:

$$ \text{df} = (\text{총 정보량}) - (\text{소모된 정보량}) $$

$$ = (IJ - 1) - [(I-1) + (J-1)] $$

$$ = IJ - 1 - I + 1 - J + 1 = IJ - I - J + 1 = (I-1)(J-1) $$

이 $(I-1)(J-1)$라는 자유도 공식은 분할표 분석에서 가장 기본적이고 중요한 결과 중 하나입니다. 이는 두 변수 사이의 '순수한' 연관성에 대한 정보가 몇 차원인지를 나타냅니다.

**결론**: $X^2$과 $G^2$ 모두 독립성 가설로부터 데이터가 얼마나 벗어났는지를 측정하며, 대표본에서는 거의 동일한 값을 줍니다. 두 통계량 모두 귀무가설 하에서 자유도가 $(I-1)(J-1)$인 카이제곱 분포를 따르므로, 계산된 값이 이 분포에서 얼마나 극단적인지를 보고(P-값) 독립성 가설을 기각할지 여부를 판단합니다.
