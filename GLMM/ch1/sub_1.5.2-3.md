피어슨 카이제곱 검정과 우도비 카이제곱 검정은 범주형 데이터 분석의 초석이 되는 매우 중요한 개념입니다. 두 검정의 철학과 차이점을 깊이 있게 설명해 드리겠습니다.

### 공통적인 목표: "내 가설이 데이터를 잘 설명하는가?"

두 검정 모두 **적합도 검정(Goodness-of-Fit Test)**의 일종입니다. 즉, 다음과 같은 질문에 답하고자 합니다.

> **"내가 세운 가설(귀무가설, $H_0$)이 실제 관측된 데이터와 얼마나 잘 들어맞는가?"**

여기서 가설은 각 범주에 대한 확률($\pi_{j0}$)을 구체적으로 명시하는 것입니다. 예를 들어, 주사위를 60번 던졌을 때 다음과 같은 데이터를 얻었다고 합시다.

*   **데이터 (관측 빈도, $n_j$)**:
    *   1의 눈: 8번
    *   2의 눈: 12번
    *   3의 눈: 9번
    *   4의 눈: 11번
    *   5의 눈: 10번
    *   6의 눈: 10번
    *   (총합 $n=60$)
*   **귀무가설 ($H_0$)**: "이 주사위는 공정하다."
    *   수학적으로: $\pi_1=\pi_2=\dots=\pi_6 = 1/6$.
*   **기대 빈도 ($\mu_j = n \times \pi_{j0}$)**:
    *   만약 주사위가 정말 공정하다면, 각 눈은 평균적으로 $60 \times (1/6) = 10$번씩 나와야 합니다. 이것이 기대 빈도입니다.

이제 두 검정은 "관측 빈도"와 "기대 빈도" 사이의 **불일치(discrepancy)**를 각자의 방식으로 측정합니다.

---

### 1.5.2 피어슨 카이제곱 검정 (Pearson Chi-Squared Test, $X^2$)

**핵심 철학**: "각 범주에서 관측값과 기대값의 차이를 제곱하여 불일치를 측정하고, 이를 모두 합산한다."

**통계량 공식:**

$$ X^2 = \sum_{j=1}^{c} \frac{(n_j - \mu_j)^2}{\mu_j} = \sum \frac{(\text{관측 빈도} - \text{기대 빈도})^2}{\text{기대 빈도}} $$

**수식 해부:**

1.  **$(n_j - \mu_j)$**: **편차 (Deviation)**
    *   각 범주에서 관측값과 기대값의 단순한 차이입니다.
    *   이 차이가 크면 클수록 가설에 불리합니다.

2.  **$(n_j - \mu_j)^2$**: **편차의 제곱 (Squared Deviation)**
    *   편차는 양수일 수도, 음수일 수도 있습니다. 이들을 그냥 더하면 서로 상쇄될 수 있으므로, 제곱을 하여 모든 불일치를 양수로 만들고, 큰 편차에 더 많은 가중치를 줍니다.

3.  **$\frac{(\dots)^2}{\mu_j}$**: **표준화된 편차의 제곱 (Standardized Squared Deviation)**
    *   이 부분이 매우 중요합니다. 편차의 제곱을 **기대 빈도($\mu_j$)로 나누어 줍니다.**
    *   **이유**: 똑같은 `(관측-기대)` 차이라도, 그 중요성은 기대 빈도의 크기에 따라 달라집니다.
        *   **예시 A**: 기대 빈도가 10인데 관측 빈도가 15라면, 차이는 5입니다. ($ (15-10)^2/10 = 2.5 $)
        *   **예시 B**: 기대 빈도가 100인데 관측 빈도가 105라면, 차이는 똑같이 5입니다. 하지만 기대치가 100인 상황에서 5의 차이는 상대적으로 작아 보입니다. ($ (105-100)^2/100 = 0.25 $)
    *   기대 빈도로 나누어 줌으로써, 각 범주에서 발생한 불일치를 **상대적인 크기**로 변환하여 공평하게 비교할 수 있게 됩니다.

4.  **$\sum (\dots)$**: **총 불일치 (Total Discrepancy)**
    *   모든 범주에 대해 계산된 표준화된 불일치를 모두 합산하여, 가설과 데이터 사이의 **전체적인 불일치 정도**를 나타내는 단일 숫자를 만듭니다.

**결론**: $X^2$ 값이 크면 클수록, 관측된 데이터가 가설로부터 멀리 떨어져 있다는 의미이므로 $H_0$를 기각할 근거가 됩니다.

---

### 1.5.3 우도비 카이제곱 검정 (Likelihood-Ratio Chi-Squared Test, $G^2$)

**핵심 철학**: "정보 이론(Information Theory)의 관점에서, 관측된 데이터 분포를 설명하는 데 '내 가설'이 '가장 완벽한 가설'에 비해 얼마나 더 많은 정보 손실을 유발하는가?"

**통계량 공식:**

$$ G^2 = 2 \sum_{j=1}^{c} n_j \log\left(\frac{n_j}{n\pi_{j0}}\right) = 2 \sum \text{관측 빈도} \times \log\left(\frac{\text{관측 빈도}}{\text{기대 빈도}}\right) $$

**수식 해부:**

이 수식은 우도비($\Lambda = l_0 / l_1$)에서 유도되었습니다.
*   **$l_1$ (최상의 설명력)**: 데이터 자체를 가장 잘 설명하는 모델은 각 범주의 확률을 표본 비율($\hat{\pi}_j = n_j/n$)로 설정하는 것입니다.
*   **$l_0$ (가설 하의 설명력)**: 귀무가설 하에서는 각 범주의 확률이 $\pi_{j0}$로 고정됩니다.
*   $G^2 = -2 \log(\Lambda)$는 이 두 설명력의 차이를 나타냅니다.

**직관적 해석:**

1.  **$\frac{n_j}{\mu_j} = \frac{\text{관측 빈도}}{\text{기대 빈도}}$**: **관측/기대 비율**
    *   이 비율이 1이면, 관측값과 기대값이 정확히 일치하여 불일치가 없습니다.
    *   이 비율이 1에서 멀어질수록 불일치가 커집니다.

2.  **$\log(\frac{n_j}{\mu_j})$**: **로그 비율**
    *   로그 함수는 비율이 1일 때($\log(1)=0$), 값이 0이 됩니다. 즉, 불일치가 없으면 기여도도 0입니다.
    *   비율이 1보다 크거나 작을수록, 로그 값의 절댓값은 커집니다. 로그 변환은 불일치의 정도를 다른 척도로 변환하는 역할을 합니다.

3.  **$n_j \log(\dots)$**: **관측 빈도로 가중치 부여**
    *   각 범주의 로그 비율에 해당 범주의 **관측 빈도($n_j$)를 곱해줍니다.**
    *   이는 관측 빈도가 높은 범주에서 발생한 불일치가, 관측 빈도가 낮은 범주에서 발생한 불일치보다 전체적인 불일치에 더 큰 영향을 미치도록 가중치를 부여하는 효과가 있습니다.

4.  **$2 \sum (\dots)$**: **총 정보 손실**
    *   모든 범주에 대해 계산된 가중치가 부여된 불일치를 모두 합산하고 2를 곱하여(수학적 유도 과정의 결과), 가설이 유발하는 **총 정보 손실량**을 나타내는 단일 숫자를 만듭니다.

**결론**: $G^2$ 값이 크면 클수록, 내 가설이 데이터를 설명하는 데 발생하는 정보 손실이 크다는 의미이므로 $H_0$를 기각할 근거가 됩니다.

---

### 두 검정의 비교 및 관계

| 특징 | 피어슨 카이제곱 검정 ($X^2$) | 우도비 카이제곱 검정 ($G^2$) |
| :--- | :--- | :--- |
| **기반 철학** | 기하학적 거리 (유클리드 거리와 유사) | 정보 이론적 거리 (정보 손실) |
| **계산 공식** | $\sum \frac{(\text{관측-기대})^2}{\text{기대}}$ | $2 \sum \text{관측} \log(\frac{\text{관측}}{\text{기대}})$ |
| **통계적 성질** | **대푯본(Large Sample)에서는 두 검정의 결과가 거의 동일**합니다. ($X^2 \approx G^2$) |
| **수렴 속도** | 일반적으로 $G^2$보다 카이제곱 분포로 더 빨리 수렴하는 것으로 알려져 있어, 상대적으로 작은 표본에서도 더 정확한 경향이 있습니다. | 상대적으로 더 큰 표본 크기를 요구할 수 있습니다. |
| **가산성** | $G^2$ 통계량은 복잡한 모델에서 분할(partitioning)이 가능하다는 이론적 장점이 있습니다. (더 고급 주제) | $X^2$은 일반적으로 이 속성을 만족하지 않습니다. |

**실용적인 결론**: 대부분의 적합도 검정 문제에서 **두 검정 중 어느 것을 사용해도 거의 동일한 결론**에 도달합니다. 역사적으로 $X^2$이 계산하기 더 쉬워서 널리 사용되었지만, 현대 컴퓨터 환경에서는 둘 다 계산이 쉽습니다. 통계학자들 사이에서는 $G^2$의 이론적 우아함(우도 원리와의 직접적 연결)을 선호하는 경향도 있습니다.
