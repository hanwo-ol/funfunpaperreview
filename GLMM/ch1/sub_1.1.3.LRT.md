### 우도비 검정의 핵심 아이디어: "제약이 얼마나 나쁜가?"

우도비 검정의 근본적인 질문은 다음과 같습니다.

> **"귀무가설($H_0$)이라는 제약을 모델에 걸었을 때, 데이터에 대한 설명력이 얼마나 나빠지는가?"**

만약 귀무가설이 사실이라면, 모델에 그 제약을 걸어도 데이터에 대한 설명력은 크게 나빠지지 않을 것입니다. 하지만 귀무가설이 사실이 아니라면, 억지로 제약을 거는 것이므로 모델의 설명력은 현저하게 떨어질 것입니다. 우도비 검정은 바로 이 "설명력이 나빠지는 정도"를 측정하여 귀무가설을 기각할지 여부를 판단하는 방법입니다.

여기서 **"모델의 설명력"**을 측정하는 지표가 바로 **우도(Likelihood)**입니다. 우도가 클수록 모델이 데이터를 더 잘 설명한다고 봅니다.

---

### 두 가지 모델의 비교

우도비 검정은 항상 두 가지 모델을 비교합니다.

1.  **전체 모델 (Full Model / Unrestricted Model)**
    *   이 모델은 귀무가설($H_0$)의 제약을 받지 않는, 더 자유롭고 일반적인 모델입니다.
    *   텍스트에서는 이 모델로부터 얻은 최대 우도를 **$l_1$** (또는 최대 로그우도를 $L_1$)이라고 부릅니다.
    *   이 값은 주어진 데이터에 대해 우리가 얻을 수 있는 **"최상의 설명력"**을 나타냅니다.

2.  **축소 모델 (Reduced Model / Restricted Model)**
    *   이 모델은 귀무가설($H_0$)의 제약을 **강제로 적용한** 모델입니다.
    *   텍스트에서는 이 모델로부터 얻은 최대 우도를 **$l_0$** (또는 최대 로그우도를 $L_0$)이라고 부릅니다.
    *   이 값은 **"$H_0$이 사실이라는 가정 하에서"** 얻을 수 있는 최상의 설명력을 나타냅니다.

**예시 (텍스트의 예)**
*   어떤 현상을 설명하는 모수가 $\beta_0$와 $\beta_1$ 두 개라고 합시다. (예: $Y = \beta_0 + \beta_1 X$)
*   우리의 귀무가설은 $H_0: \beta_1 = 0$ 입니다. (즉, "X는 Y에 영향을 주지 않는다")
*   **전체 모델**: $\beta_0$와 $\beta_1$을 모두 자유롭게 추정하여 최대우도 **$l_1$**을 계산합니다.
*   **축소 모델**: $\beta_1=0$이라는 제약을 강제로 적용하고 (즉, $Y = \beta_0$ 모델), $\beta_0$만을 추정하여 최대우도 **$l_0$**를 계산합니다.

**$l_1$과 $l_0$의 관계**
축소 모델은 전체 모델에 제약을 가한 것이므로, 축소 모델의 설명력($l_0$)이 전체 모델의 설명력($l_1$)보다 좋을 수는 절대로 없습니다. 따라서 항상 **$l_0 \le l_1$** 입니다.

---

### 우도비(Likelihood Ratio) 통계량

이제 두 모델의 설명력을 비교할 차례입니다. 가장 자연스러운 방법은 비율을 보는 것입니다.

*   **우도비(Likelihood Ratio), $\Lambda$**:

$$ \Lambda = \frac{l_0}{l_1} = \frac{\text{축소 모델의 최대 우도}}{\text{전체 모델의 최대 우도}} $$

*   이 비율 $\Lambda$는 항상 0과 1 사이의 값을 가집니다 ($0 \le \Lambda \le 1$).
    *   **$\Lambda$가 1에 가깝다**: $l_0$와 $l_1$이 거의 같다. 즉, 귀무가설($H_0$)이라는 제약을 걸어도 설명력이 거의 나빠지지 않았다. 이는 **$H_0$을 기각할 근거가 부족함**을 시사합니다.
    *   **$\Lambda$가 0에 가깝다**: $l_0$가 $l_1$보다 현저하게 작다. 즉, $H_0$이라는 제약을 걸었더니 설명력이 크게 나빠졌다. 이는 $H_0$이 사실이 아닐 가능성이 높으며, **$H_0$을 기각할 강력한 근거**가 됨을 시사합니다.

### 최종 검정 통계량: $-2 \log \Lambda$

우도비 $\Lambda$ 자체를 사용하는 것은 통계적으로 다루기 어렵습니다. 놀랍게도, 여기에 간단한 수학적 변환을 가하면 우리가 잘 아는 분포를 따르게 됩니다. 이것이 Samuel S. Wilks의 위대한 발견입니다.

> **검정 통계량**:

$$ -2 \log \Lambda = -2 \log\left(\frac{l_0}{l_1}\right) $$

로그의 성질($\log(a/b) = \log(a) - \log(b)$)을 이용하면 다음과 같이 변형할 수 있습니다.
$$ = -2 (\log(l_0) - \log(l_1)) = -2 (L_0 - L_1) $$
여기서 $L_0, L_1$은 각각의 최대 로그우도입니다.

**이 변환의 장점:**

1.  **카이제곱($\chi^2$) 분포**: 귀무가설이 사실일 때, 이 $-2 \log \Lambda$ 통계량은 표본 크기($n$)가 충분히 크면 근사적으로 **카이제곱($\chi^2$) 분포**를 따릅니다. 카이제곱 분포는 통계학에서 매우 잘 알려진 분포이므로, 우리가 계산한 통계량이 얼마나 극단적인 값인지(P-값)를 쉽게 알 수 있습니다.
2.  **해석의 용이성**:
    *   $\Lambda$가 1에 가까우면($H_0$ 지지), $\log \Lambda$는 0에 가깝고, $-2 \log \Lambda$도 **0에 가까워집니다**.
    *   $\Lambda$가 0에 가까우면($H_0$ 기각), $\log \Lambda$는 음의 무한대에 가까워지고, $-2 \log \Lambda$는 **매우 큰 양수 값**이 됩니다.
    *   따라서, 우리는 $-2 \log \Lambda$ 값이 크면 클수록 $H_0$을 기각하게 됩니다.

**자유도 (Degrees of Freedom, df)**
이때 따르는 카이제곱 분포의 자유도는 매우 중요하며, 다음과 같이 결정됩니다.

> **df = (전체 모델의 자유 모수 개수) - (축소 모델의 자유 모수 개수)**

이는 귀무가설이 모델에 가하는 **"제약의 개수"**와 같습니다. 위의 예시($H_0: \beta_1=0$)에서는 전체 모델은 $\beta_0, \beta_1$ 두 개를 추정하고 축소 모델은 $\beta_0$ 한 개만 추정하므로, 자유도는 $2-1=1$이 됩니다.

### 요약

*   **우도비 검정**은 **귀무가설($H_0$)이라는 제약**이 모델의 **설명력(우도)**을 얼마나 감소시키는지를 측정합니다.
*   **전체 모델(제약 없음)**과 **축소 모델(제약 있음)**의 최대 우도를 각각 계산하여 그 **비율($\Lambda = l_0/l_1$)**을 구합니다.
*   이 비율에 **$-2 \log$** 변환을 취한 통계량($-2 \log \Lambda$)은 귀무가설 하에서 **카이제곱 분포**를 따릅니다.
*   계산된 통계량이 **크면 클수록** 귀무가설에 불리한 증거가 되며, P-값을 계산하여 가설을 기각할지 결정합니다.
*   이 방법은 매우 일반적이어서 거의 모든 종류의 통계 모델에 적용할 수 있는 강력한 도구입니다.
